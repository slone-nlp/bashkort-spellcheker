{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb558e3",
   "metadata": {},
   "source": [
    "Inspired by https://github.com/lingjzhu/CharsiuG2P. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73ae3c",
   "metadata": {},
   "source": [
    "* Baseline (just a small byt5): 0.387 reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32141927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3345d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('charsiu/g2p_multilingual_byT5_tiny_8_layers_100')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/byt5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ee9a066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ˈtʃɑɹ', 'ˈsiu', 'ˈɪs', 'ˈɑ', 'ˈkæntoʊˌniz', 'ˈstaɪɫ', 'ˈɔf', 'ˈbɑɹbɪkjud', 'ˈpɔɹk']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenized English words\n",
    "words = ['Char', 'siu', 'is', 'a', 'Cantonese', 'style', 'of', 'barbecued', 'pork']\n",
    "words = ['<eng-us>: '+i for i in words]\n",
    "\n",
    "out = tokenizer(words,padding=True,add_special_tokens=False,return_tensors='pt')\n",
    "\n",
    "preds = model.generate(**out,num_beams=1) # We do not find beam search helpful. Greedy decoding is enough. \n",
    "phones = tokenizer.batch_decode(preds.tolist(),skip_special_tokens=True)\n",
    "print(phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90fbdc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ɡəspɐdak', 'ja', 'nʲe', 'jeɫ', 'ʂɛsʲtʲ', 'dʲnʲej', 't͡sɛɫɨx', 'tsʰɪŋ˧˥', 'dʲnʲej']\n",
      "Wall time: 125 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "words = 'Господа, я не ел шесть дней целых 6 дней'.split()\n",
    "words = ['<rus>: '+i for i in words]\n",
    "\n",
    "out = tokenizer(words, padding=True, add_special_tokens=False, return_tensors='pt')\n",
    "\n",
    "preds = model.generate(**out,num_beams=1) # We do not find beam search helpful. Greedy decoding is enough. \n",
    "phones = tokenizer.batch_decode(preds.tolist(),skip_special_tokens=True)\n",
    "print(phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f160efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ð', '¿', 'Ñ', '\\x80', 'Ð', '¸', 'Ð', '²', 'Ð', 'µ', 'Ñ', '\\x82', '</s>']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer('привет').input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd21ae",
   "metadata": {},
   "source": [
    "# Loading the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ee7362",
   "metadata": {},
   "source": [
    "## The original sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75462b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4aec2533",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trash</th>\n",
       "      <th>clean</th>\n",
       "      <th>trash2</th>\n",
       "      <th>clean2</th>\n",
       "      <th>distance</th>\n",
       "      <th>normalized_distance</th>\n",
       "      <th>split</th>\n",
       "      <th>edit_max_cldiff</th>\n",
       "      <th>edit_max_lendiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Шунда ук әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...</td>\n",
       "      <td>Шунда уҡ әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...</td>\n",
       "      <td>Шунда ук әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...</td>\n",
       "      <td>Шунда уҡ әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Унан беҙ өсөбөҙ ҙә ултырғыстарға ултырабыҙ.</td>\n",
       "      <td>Унан беҙ әсәбеҙ ҙә ултырғыстарға ултырабыҙ.</td>\n",
       "      <td>Унан беҙ өсөбөҙ ҙә ултырғыстарға ултырабыҙ.</td>\n",
       "      <td>Унан беҙ әсәбеҙ ҙә ултырғыстарға ултырабыҙ.</td>\n",
       "      <td>3</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>«Иҫән-Һау ғына тороғоҙ инде», - тип бышылдай у...</td>\n",
       "      <td>«Иҫән-һау ғына тороғоҙ инде», - тип бышылдай у...</td>\n",
       "      <td>«Иҫән-Һау ғына тороғоҙ инде», - тип бышылдай у...</td>\n",
       "      <td>«Иҫән-һау ғына тороғоҙ инде», - тип бышылдай у...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>dev</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Минең генә бер кешем дә юҡ, тип шунда уҡ танау...</td>\n",
       "      <td>Минең генә бер кешем дә юҡ, - тип шунда уҡ тан...</td>\n",
       "      <td>Минең генә бер кешем дә юҡ, тип шунда уҡ танау...</td>\n",
       "      <td>Минең генә бер кешем дә юҡ, - тип шунда уҡ тан...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...</td>\n",
       "      <td>Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...</td>\n",
       "      <td>Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...</td>\n",
       "      <td>Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23886</th>\n",
       "      <td>Эҫтәрендә бүре үк оломаһа ла, эттәр шыңшый баш...</td>\n",
       "      <td>Эстәрендә бүре үк оломаһа ла, эттәр шыңшый баш...</td>\n",
       "      <td>Эҫтәрендә бүре үк оломаһа ла, эттәр шыңшый баш...</td>\n",
       "      <td>Эстәрендә бүре үк оломаһа ла, эттәр шыңшый баш...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>dev</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23887</th>\n",
       "      <td>Үткән йәйҙә яман томра көндө Кәҙерғол төбәгенд...</td>\n",
       "      <td>Үткән йәйҙә яман томра көндө Ҡәҙерғол төбәгенд...</td>\n",
       "      <td>Үткән йәйҙә яман томра көндө Кәҙерғол төбәгенд...</td>\n",
       "      <td>Үткән йәйҙә яман томра көндө Ҡәҙерғол төбәгенд...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23888</th>\n",
       "      <td>Кайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...</td>\n",
       "      <td>Ҡайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...</td>\n",
       "      <td>Кайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...</td>\n",
       "      <td>Ҡайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23889</th>\n",
       "      <td>Кыш урталарында бер көн Әбдрәшит ат аҙбарынан ...</td>\n",
       "      <td>Ҡыш урталарында бер көн Әбдрәшит ат аҙбарынан ...</td>\n",
       "      <td>Кыш урталарында бер көн Әбдрәшит ат аҙбарынан ...</td>\n",
       "      <td>Ҡыш урталарында бер көн Әбдрәшит ат аҙбарынан ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23890</th>\n",
       "      <td>Шатлыҡ тигәнебеҙ юғалған салбар ҡупшы ғына ите...</td>\n",
       "      <td>Шатлыҡ тигәнебеҙ юғалған салбар - ҡупшы ғына и...</td>\n",
       "      <td>Шатлыҡ тигәнебеҙ юғалған салбар ҡупшы ғына ите...</td>\n",
       "      <td>Шатлыҡ тигәнебеҙ юғалған салбар - ҡупшы ғына и...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23891 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   trash  \\\n",
       "0      Шунда ук әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...   \n",
       "1            Унан беҙ өсөбөҙ ҙә ултырғыстарға ултырабыҙ.   \n",
       "2      «Иҫән-Һау ғына тороғоҙ инде», - тип бышылдай у...   \n",
       "3      Минең генә бер кешем дә юҡ, тип шунда уҡ танау...   \n",
       "4      Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...   \n",
       "...                                                  ...   \n",
       "23886  Эҫтәрендә бүре үк оломаһа ла, эттәр шыңшый баш...   \n",
       "23887  Үткән йәйҙә яман томра көндө Кәҙерғол төбәгенд...   \n",
       "23888  Кайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...   \n",
       "23889  Кыш урталарында бер көн Әбдрәшит ат аҙбарынан ...   \n",
       "23890  Шатлыҡ тигәнебеҙ юғалған салбар ҡупшы ғына ите...   \n",
       "\n",
       "                                                   clean  \\\n",
       "0      Шунда уҡ әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...   \n",
       "1            Унан беҙ әсәбеҙ ҙә ултырғыстарға ултырабыҙ.   \n",
       "2      «Иҫән-һау ғына тороғоҙ инде», - тип бышылдай у...   \n",
       "3      Минең генә бер кешем дә юҡ, - тип шунда уҡ тан...   \n",
       "4      Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...   \n",
       "...                                                  ...   \n",
       "23886  Эстәрендә бүре үк оломаһа ла, эттәр шыңшый баш...   \n",
       "23887  Үткән йәйҙә яман томра көндө Ҡәҙерғол төбәгенд...   \n",
       "23888  Ҡайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...   \n",
       "23889  Ҡыш урталарында бер көн Әбдрәшит ат аҙбарынан ...   \n",
       "23890  Шатлыҡ тигәнебеҙ юғалған салбар - ҡупшы ғына и...   \n",
       "\n",
       "                                                  trash2  \\\n",
       "0      Шунда ук әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...   \n",
       "1            Унан беҙ өсөбөҙ ҙә ултырғыстарға ултырабыҙ.   \n",
       "2      «Иҫән-Һау ғына тороғоҙ инде», - тип бышылдай у...   \n",
       "3      Минең генә бер кешем дә юҡ, тип шунда уҡ танау...   \n",
       "4      Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...   \n",
       "...                                                  ...   \n",
       "23886  Эҫтәрендә бүре үк оломаһа ла, эттәр шыңшый баш...   \n",
       "23887  Үткән йәйҙә яман томра көндө Кәҙерғол төбәгенд...   \n",
       "23888  Кайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...   \n",
       "23889  Кыш урталарында бер көн Әбдрәшит ат аҙбарынан ...   \n",
       "23890  Шатлыҡ тигәнебеҙ юғалған салбар ҡупшы ғына ите...   \n",
       "\n",
       "                                                  clean2  distance  \\\n",
       "0      Шунда уҡ әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...         1   \n",
       "1            Унан беҙ әсәбеҙ ҙә ултырғыстарға ултырабыҙ.         3   \n",
       "2      «Иҫән-һау ғына тороғоҙ инде», - тип бышылдай у...         1   \n",
       "3      Минең генә бер кешем дә юҡ, - тип шунда уҡ тан...         2   \n",
       "4      Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...         1   \n",
       "...                                                  ...       ...   \n",
       "23886  Эстәрендә бүре үк оломаһа ла, эттәр шыңшый баш...         1   \n",
       "23887  Үткән йәйҙә яман томра көндө Ҡәҙерғол төбәгенд...         1   \n",
       "23888  Ҡайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...         1   \n",
       "23889  Ҡыш урталарында бер көн Әбдрәшит ат аҙбарынан ...         1   \n",
       "23890  Шатлыҡ тигәнебеҙ юғалған салбар - ҡупшы ғына и...         2   \n",
       "\n",
       "       normalized_distance  split  edit_max_cldiff  edit_max_lendiff  \n",
       "0                 0.015385  train                1                 0  \n",
       "1                 0.069767   test                1                 0  \n",
       "2                 0.014085    dev                1                 0  \n",
       "3                 0.029412  train                0                 0  \n",
       "4                 0.012500  train                1                 0  \n",
       "...                    ...    ...              ...               ...  \n",
       "23886             0.020000    dev                1                 0  \n",
       "23887             0.009524  train                1                 0  \n",
       "23888             0.020000  train                1                 0  \n",
       "23889             0.009174  train                1                 0  \n",
       "23890             0.021053  train                0                 0  \n",
       "\n",
       "[23891 rows x 9 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig = pd.read_csv('../data/spellchecker_dataset_split.tsv', sep='\\t')\n",
    "df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecfb77d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14382, 9)\n",
      "(14171, 9)\n",
      "(14085, 9)\n"
     ]
    }
   ],
   "source": [
    "df_orig_train = df_orig[(df_orig.split=='train')]\n",
    "print(df_orig_train.shape)\n",
    "\n",
    "df_orig_train = df_orig_train[df_orig_train.edit_max_cldiff <= 3]\n",
    "print(df_orig_train.shape)\n",
    "df_orig_train = df_orig_train[df_orig_train.edit_max_lendiff <= 1].copy()\n",
    "print(df_orig_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "48011163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4611, 9)\n"
     ]
    }
   ],
   "source": [
    "df_orig_dev = df_orig[(df_orig.split=='dev') & (df_orig.edit_max_cldiff <= 3) & (df_orig.edit_max_lendiff <= 1)]\n",
    "print(df_orig_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "b78c2c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_lens = pd.Series([len(s) for s in tokenizer(df_orig_train.trash2.tolist())['input_ids']])\n",
    "new_lens = pd.Series([len(s) for s in tokenizer(df_orig_train.clean2.tolist())['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "92ad1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import QuantileRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "9bfee682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999995049014 2.000000004774404\n",
      "0.9878048779218795 -1.1097560969380593\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "qr = QuantileRegressor(quantile=0.99).fit(old_lens.head(n).to_frame(), new_lens.head(n))\n",
    "print(qr.coef_[0], qr.intercept_)\n",
    "qr = QuantileRegressor(quantile=0.01).fit(old_lens.head(n).to_frame(), new_lens.head(n))\n",
    "print(qr.coef_[0], qr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362970c4",
   "metadata": {},
   "source": [
    "## Artificial replacements (todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c664b",
   "metadata": {},
   "source": [
    "Clone https://github.com/nevmenandr/bashkir-corpus/ nearby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df01cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4113a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b65642c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../bashkir-corpus/public_domain'\n",
    "for fname in os.listdir(path):\n",
    "    if fname.endswith('.txt'):\n",
    "        with open(path + '/' + fname, 'r') as f:\n",
    "            clean_texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eb658ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036392cc38ed476e8f7e14f6479c3f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025888c51b4e4bd9b3551c97f4ec7554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df97c743e88460e923444f3896f4af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a57d6cc2fd444a3a4215c05a8086a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4427840c3d334510924b9f9ec8fdfc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea36ff986927408fb4df63781826320a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db217bfeed4a46919587cfb7cb2d3bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4808789881d34551b03f1d8c8fec7621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3c3c714ee94175bcfed1b14edfc001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2002 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = '../../bashkir-corpus/shuffled_texts'\n",
    "for dirname in tqdm(os.listdir(path)):\n",
    "    path1 = path + '/' + dirname\n",
    "    for fname in tqdm(os.listdir(path1)):\n",
    "        if fname.endswith('.txt'):\n",
    "            with open(path1 + '/' + fname, 'r') as f:\n",
    "                clean_texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8f59f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47112\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e7622fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "903c6155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0f031100e9481da7eb1ff29f122f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1668613\n"
     ]
    }
   ],
   "source": [
    "clean_sents = [sent.text for text in tqdm(clean_texts) for sent  in razdel.sentenize(text)]\n",
    "print(len(clean_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef827a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd9f5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21226e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_orig_train.copy().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb5ed680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['trash', 'clean', 'trash2', 'clean2', 'distance', 'normalized_distance',\n",
       "       'split', 'edit_max_cldiff', 'edit_max_lendiff'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0486b07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c11b296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1cc0302",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d5a4486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "report_steps = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8448522",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "19939f0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb4e039f8a148d9bec3fbf4df2820bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc9b48796864a3c9cafc84fadddfb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14085 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 2.177804946899414\n",
      "step 200 loss 2.069809110760689\n",
      "step 400 loss 1.8379772818088531\n",
      "step 600 loss 1.7224777126312256\n",
      "step 800 loss 1.6701817506551742\n",
      "step 1000 loss 1.6214507526159287\n",
      "step 1200 loss 1.5839875054359436\n",
      "step 1400 loss 1.5325424307584763\n",
      "step 1600 loss 1.552054591178894\n",
      "step 1800 loss 1.4793325781822204\n",
      "step 2000 loss 1.4576475125551225\n",
      "step 2200 loss 1.4384098660945892\n",
      "step 2400 loss 1.4360230815410615\n",
      "step 2600 loss 1.380750117301941\n",
      "step 2800 loss 1.3458383771777154\n",
      "step 3000 loss 1.2762595593929291\n",
      "step 3200 loss 1.2012317070364953\n",
      "step 3400 loss 1.0861580811440945\n",
      "step 3600 loss 0.9555415490269661\n",
      "step 3800 loss 0.8347638335824012\n",
      "step 4000 loss 0.7024352759122848\n",
      "step 4200 loss 0.6485723718255758\n",
      "step 4400 loss 0.5694224166870118\n",
      "step 4600 loss 0.5382870144769549\n",
      "step 4800 loss 0.49289742462337016\n",
      "step 5000 loss 0.42268219163641335\n",
      "step 5200 loss 0.4472760313563049\n",
      "step 5400 loss 0.36272449519485234\n",
      "step 5600 loss 0.38548546240665016\n",
      "step 5800 loss 0.3482202092371881\n",
      "step 6000 loss 0.3222203397192061\n",
      "step 6200 loss 0.3141317765414715\n",
      "step 6400 loss 0.2796920345257968\n",
      "step 6600 loss 0.2704834652505815\n",
      "step 6800 loss 0.2720709628518671\n",
      "step 7000 loss 0.2366212583426386\n",
      "step 7200 loss 0.23058939633890987\n",
      "step 7400 loss 0.24039069537073374\n",
      "step 7600 loss 0.24084714008495212\n",
      "step 7800 loss 0.22511958005372434\n",
      "step 8000 loss 0.2209953525941819\n",
      "step 8200 loss 0.21962643986567854\n",
      "step 8400 loss 0.21488282022066416\n",
      "step 8600 loss 0.18746032911352814\n",
      "step 8800 loss 0.2074439118988812\n",
      "step 9000 loss 0.19323114034719766\n",
      "step 9200 loss 0.1955099921952933\n",
      "step 9400 loss 0.2004303864017129\n",
      "step 9600 loss 0.1530774987535551\n",
      "step 9800 loss 0.1564723615720868\n",
      "step 10000 loss 0.16336254093330355\n",
      "step 10200 loss 0.16060082061216235\n",
      "step 10400 loss 0.15967977851163595\n",
      "step 10600 loss 0.1627102005854249\n",
      "step 10800 loss 0.14375511622522028\n",
      "step 11000 loss 0.13673747112276033\n",
      "step 11200 loss 0.1437728476943448\n",
      "step 11400 loss 0.14313504349440337\n",
      "step 11600 loss 0.12936027164338157\n",
      "step 11800 loss 0.15081047269981354\n",
      "step 12000 loss 0.13518507393542678\n",
      "step 12200 loss 0.12002305221976713\n",
      "step 12400 loss 0.12238109015859663\n",
      "step 12600 loss 0.1307052593259141\n",
      "step 12800 loss 0.19188067722832783\n",
      "step 13000 loss 0.15552537811687217\n",
      "step 13200 loss 0.1339958413434215\n",
      "step 13400 loss 0.1559347482584417\n",
      "step 13600 loss 0.11590553849004209\n",
      "step 13800 loss 0.13123406535713003\n",
      "step 14000 loss 0.12399798754137009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769e49e16d244c82bbc5d2f88d62c85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14085 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 0.13245072779944167\n",
      "step 200 loss 0.11774906099308283\n",
      "step 400 loss 0.11374102583387867\n",
      "step 600 loss 0.10535082482267172\n",
      "step 800 loss 0.0936808612185996\n",
      "step 1000 loss 0.11489498475799337\n",
      "step 1200 loss 0.10599727109540254\n",
      "step 1400 loss 0.11210539936088025\n",
      "step 1600 loss 0.10640442902920767\n",
      "step 1800 loss 0.11622506901388988\n",
      "step 2000 loss 0.10328083934262394\n",
      "step 2200 loss 0.09906636469066143\n",
      "step 2400 loss 0.09995371657656506\n",
      "step 2600 loss 0.09410851504188031\n",
      "step 2800 loss 0.10241852417355403\n",
      "step 3000 loss 0.0967579515860416\n",
      "step 3200 loss 0.0908314854244236\n",
      "step 3400 loss 0.08893186592729763\n",
      "step 3600 loss 0.09791561346268281\n",
      "step 3800 loss 0.10585454828105867\n",
      "step 4000 loss 0.08574427725980058\n",
      "step 4200 loss 0.09295185362454504\n",
      "step 4400 loss 0.11120611437479966\n",
      "step 4600 loss 0.08568946803221479\n",
      "step 4800 loss 0.09021008684416301\n",
      "step 5000 loss 0.0829883080883883\n",
      "step 5200 loss 0.09485960476798937\n",
      "step 5400 loss 0.0954258233308792\n",
      "step 5600 loss 0.07746028521331028\n",
      "step 5800 loss 0.09572447252692655\n",
      "step 6000 loss 0.08624106197850778\n",
      "step 6200 loss 0.08412022262695246\n",
      "step 6400 loss 0.07936076933518052\n",
      "step 6600 loss 0.0904259440698661\n",
      "step 6800 loss 0.07634287111461163\n",
      "step 7000 loss 0.08801100309705362\n",
      "step 7200 loss 0.07708291525486857\n",
      "step 7400 loss 0.08461947198724375\n",
      "step 7600 loss 0.07989645083900541\n",
      "step 7800 loss 0.08202309836167843\n",
      "step 8000 loss 0.08018868679879233\n",
      "step 8200 loss 0.08478993501048535\n",
      "step 8400 loss 0.07303443732205778\n",
      "step 8600 loss 0.07961176762823015\n",
      "step 8800 loss 0.07565565353259444\n",
      "step 9000 loss 0.08941391478525475\n",
      "step 9200 loss 0.0906122556317132\n",
      "step 9400 loss 0.08318271329859271\n",
      "step 9600 loss 0.0738566016277764\n",
      "step 9800 loss 0.07399251614464447\n",
      "step 10000 loss 0.09038194051245227\n",
      "step 10200 loss 0.08962373114656658\n",
      "step 10400 loss 0.07684284735936671\n",
      "step 10600 loss 0.07527180433971807\n",
      "step 10800 loss 0.064188558437163\n",
      "step 11000 loss 0.07311372624943033\n",
      "step 11200 loss 0.08274829025147483\n",
      "step 11400 loss 0.06895732713746838\n",
      "step 11600 loss 0.07930820307927205\n",
      "step 11800 loss 0.0724869601521641\n",
      "step 12000 loss 0.07802178230602294\n",
      "step 12200 loss 0.08580104969907552\n",
      "step 12400 loss 0.08224257977912203\n",
      "step 12600 loss 0.08278959391755052\n",
      "step 12800 loss 0.05737003486487083\n",
      "step 13000 loss 0.06366227235761471\n",
      "step 13200 loss 0.06499014854314737\n",
      "step 13400 loss 0.07831124615273438\n",
      "step 13600 loss 0.05942823493271135\n",
      "step 13800 loss 0.06034335161675699\n",
      "step 14000 loss 0.06054833389818668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2844bfab78342fb9cabc227a4b8ed03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14085 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 0.06362137123243883\n",
      "step 200 loss 0.06112854303559288\n",
      "step 400 loss 0.07395392916398123\n",
      "step 600 loss 0.07435652455198578\n",
      "step 800 loss 0.0640893696132116\n",
      "step 1000 loss 0.05565943330875598\n",
      "step 1200 loss 0.055610723305726424\n",
      "step 1400 loss 0.06676544181886129\n",
      "step 1600 loss 0.05976070774486288\n",
      "step 1800 loss 0.07026257760066074\n",
      "step 2000 loss 0.06341632137075066\n",
      "step 2200 loss 0.057561173638096076\n",
      "step 2400 loss 0.07754234740044921\n",
      "step 2600 loss 0.05814845366985537\n",
      "step 2800 loss 0.06484924742486328\n",
      "step 3000 loss 0.06175520313554443\n",
      "step 3200 loss 0.06453328551724553\n",
      "step 3400 loss 0.062041014791466294\n",
      "step 3600 loss 0.07167983417981304\n",
      "step 3800 loss 0.059126918840920556\n",
      "step 4000 loss 0.0716935655917041\n",
      "step 4200 loss 0.06970738740405068\n",
      "step 4400 loss 0.05854274679441005\n",
      "step 4600 loss 0.06972480339929461\n",
      "step 4800 loss 0.05252863458939828\n",
      "step 5000 loss 0.06120751568116248\n",
      "step 5200 loss 0.053280472772894424\n",
      "step 5400 loss 0.06895037763519213\n",
      "step 5600 loss 0.05535368068725802\n",
      "step 5800 loss 0.051950095545616935\n",
      "step 6000 loss 0.06672217068029568\n",
      "step 6200 loss 0.0635786947235465\n",
      "step 6400 loss 0.06207610051613301\n",
      "step 6600 loss 0.05603109741117805\n",
      "step 6800 loss 0.048760122342500836\n",
      "step 7000 loss 0.04930290518619586\n",
      "step 7200 loss 0.06098636267532129\n",
      "step 7400 loss 0.05831931939464994\n",
      "step 7600 loss 0.05806527672801167\n",
      "step 7800 loss 0.05842395702085924\n",
      "step 8000 loss 0.05806975133600645\n",
      "step 8200 loss 0.05464660810306668\n",
      "step 8400 loss 0.05802057006279938\n",
      "step 8600 loss 0.05922988084028475\n",
      "step 8800 loss 0.06429410144570284\n",
      "step 9000 loss 0.053859888495644555\n",
      "step 9200 loss 0.05584110677591525\n",
      "step 9400 loss 0.056927298727678134\n",
      "step 9600 loss 0.058622303490992636\n",
      "step 9800 loss 0.05598052219604142\n",
      "step 10000 loss 0.054376808687811716\n",
      "step 10200 loss 0.05635588766133878\n",
      "step 10400 loss 0.0455039122980088\n",
      "step 10600 loss 0.057187587133375926\n",
      "step 10800 loss 0.05873520816443488\n",
      "step 11000 loss 0.06247301410418004\n",
      "step 11200 loss 0.048889058770146224\n",
      "step 11400 loss 0.05116844617004972\n",
      "step 11600 loss 0.04804684547241777\n",
      "step 11800 loss 0.05192629501339979\n",
      "step 12000 loss 0.05822503163479269\n",
      "step 12200 loss 0.06286686136387289\n",
      "step 12400 loss 0.045810752819525076\n",
      "step 12600 loss 0.060670175241539254\n",
      "step 12800 loss 0.05420134997169953\n",
      "step 13000 loss 0.053973833579802885\n",
      "step 13200 loss 0.06302938831970095\n",
      "step 13400 loss 0.05575196301797405\n",
      "step 13600 loss 0.05109119032160379\n",
      "step 13800 loss 0.056545422105118634\n",
      "step 14000 loss 0.05225954955909401\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in trange(3):\n",
    "    shuffled = df_train.sample(frac=1.0)\n",
    "    for i in trange(0, shuffled.shape[0], bs):\n",
    "        batch = shuffled.iloc[i:i+bs]\n",
    "        x = tokenizer(batch.trash2.tolist(), padding=True, return_tensors='pt').to(model.device)\n",
    "        y = tokenizer(batch.clean2.tolist(), padding=True, return_tensors='pt').to(model.device)\n",
    "        \n",
    "        y.input_ids[y.input_ids == 0] = -100\n",
    "        loss = model(\n",
    "            input_ids=x.input_ids,\n",
    "            attention_mask=x.attention_mask,\n",
    "            labels=y.input_ids,\n",
    "            decoder_attention_mask=y.attention_mask,\n",
    "            return_dict=True\n",
    "        ).loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "        if i % report_steps == 0:\n",
    "            print('step', i, 'loss', np.mean(losses[-report_steps:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a80e56cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = None\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30ed8db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(losses).ewm(100).mean().plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "78334c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b424ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3d914d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trash                    Һаумы, нәсәлник! тип татлы йылмайып, ишек төбөндә туҡтап ҡалды ул. Был ваҡытһыҙ йөрөгән ялағайҙы күргәс, старшинаның да, урядниктың да кәйефе боҙолоп ҡуйҙы.\n",
       "clean                  Һаумы, нәсәлник! - тип татлы йылмайып, ишек төбөндә туҡтап ҡалды ул. Был ваҡытһыҙ йөрөгән ялағайҙы күргәс, старшинаның да, урядниктың да кәйефе боҙолоп ҡуйҙы.\n",
       "trash2                   Һаумы, нәсәлник! тип татлы йылмайып, ишек төбөндә туҡтап ҡалды ул. Был ваҡытһыҙ йөрөгән ялағайҙы күргәс, старшинаның да, урядниктың да кәйефе боҙолоп ҡуйҙы.\n",
       "clean2                 Һаумы, нәсәлник! - тип татлы йылмайып, ишек төбөндә туҡтап ҡалды ул. Был ваҡытһыҙ йөрөгән ялағайҙы күргәс, старшинаның да, урядниктың да кәйефе боҙолоп ҡуйҙы.\n",
       "distance                                                                                                                                                                            2\n",
       "normalized_distance                                                                                                                                                         0.0126582\n",
       "split                                                                                                                                                                             dev\n",
       "edit_max_cldiff                                                                                                                                                                     0\n",
       "edit_max_lendiff                                                                                                                                                                    0\n",
       "Name: 13600, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = df_orig_dev.sample(1).iloc[0]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "abc7a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "88e8dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix(text, num_beams=1, max_length='auto', min_length='auto', **kwargs):\n",
    "    out = tokenizer(text, padding=True, return_tensors='pt').to(model.device)\n",
    "    n = out.input_ids.shape[1]\n",
    "    if max_length == 'auto':\n",
    "        max_length = int(n * 1.02 + 4)\n",
    "    if min_length == 'auto':\n",
    "        min_length = max(1, int(n * 0.98 - 4))\n",
    "    preds = model.generate(**out,num_beams=num_beams, max_length=max_length, min_length=min_length, **kwargs)\n",
    "    result = tokenizer.decode(preds[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "46c68ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Һаумы, нәсәлник! тип татлы йылмайып, ишек төбөндә туҡтап ҡалды ул. Был ваҡытһыҙ йөрөгән ялағайҙы күргәс, старшинаның да, урядниктың да кәйефе боҙолоп ҡуйҙы.'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = fix(row.trash2)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d6548da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(textdistance.levenshtein.distance(row.trash2, row.clean2))\n",
    "print(textdistance.levenshtein.distance(result, row.clean2))\n",
    "print(textdistance.levenshtein.distance(row.trash2, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "98c3d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_small = df_orig_dev.sample(100, random_state=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e5a65ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d257a73b7643aab3d34160fc19d30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_small['fixed'] = [fix(text) for text in tqdm(dev_small.trash2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "90eb756b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdce894c00446e3a2ab5fbba9210e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_small['fixed2'] = [fix(text, num_beams=2) for text in tqdm(dev_small.trash2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "adbc72ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a940a2df9e485f90d06299fc183755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_small['fixed5'] = [fix(text, num_beams=5) for text in tqdm(dev_small.trash2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac66890",
   "metadata": {},
   "source": [
    "in fact, there is no difference between beam of 2 and 5, but 2 is better than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bec2bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_small['change_amount'] = dev_small.apply(lambda row: textdistance.levenshtein.distance(row.trash2, row.fixed5), axis=1)\n",
    "dev_small['new_diff'] = dev_small.apply(lambda row: textdistance.levenshtein.distance(row.clean2, row.fixed5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "db536dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distance               1.2900\n",
       "normalized_distance    0.0175\n",
       "edit_max_cldiff        0.4600\n",
       "edit_max_lendiff       0.0300\n",
       "change_amount          8.9100\n",
       "new_diff               8.9200\n",
       "dtype: float64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_small.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0d2deef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>normalized_distance</th>\n",
       "      <th>edit_max_cldiff</th>\n",
       "      <th>edit_max_lendiff</th>\n",
       "      <th>new_diff</th>\n",
       "      <th>profit</th>\n",
       "      <th>reduction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change_amount</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.010676</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.290323</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.142857</td>\n",
       "      <td>0.038588</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.044604</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.037050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>-24.000000</td>\n",
       "      <td>-24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>-42.000000</td>\n",
       "      <td>-7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>-76.000000</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.007905</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>-118.000000</td>\n",
       "      <td>-59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>-126.000000</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>-156.000000</td>\n",
       "      <td>-156.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>-249.000000</td>\n",
       "      <td>-249.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               distance  normalized_distance  edit_max_cldiff  \\\n",
       "change_amount                                                   \n",
       "0              0.957447             0.010676         0.404255   \n",
       "1              1.290323             0.021056         0.612903   \n",
       "2              2.142857             0.038588         0.428571   \n",
       "3              3.000000             0.044604         0.250000   \n",
       "4              3.000000             0.037050         0.000000   \n",
       "7              0.000000             0.000000         0.000000   \n",
       "15             0.000000             0.000000         0.000000   \n",
       "25             1.000000             0.018182         1.000000   \n",
       "53             6.000000             0.029703         0.000000   \n",
       "76             0.000000             0.000000         0.000000   \n",
       "119            2.000000             0.007905         1.000000   \n",
       "126            0.000000             0.000000         0.000000   \n",
       "156            1.000000             0.006944         1.000000   \n",
       "249            1.000000             0.010101         1.000000   \n",
       "\n",
       "               edit_max_lendiff    new_diff      profit   reduction  \n",
       "change_amount                                                        \n",
       "0                      0.021277    0.957447    0.000000    0.000000  \n",
       "1                      0.032258    0.387097    0.903226    0.700000  \n",
       "2                      0.142857    0.428571    1.714286    0.800000  \n",
       "3                      0.000000    1.500000    1.500000    0.500000  \n",
       "4                      0.000000    1.000000    2.000000    0.666667  \n",
       "7                      0.000000    7.000000   -7.000000        -inf  \n",
       "15                     0.000000   15.000000  -15.000000        -inf  \n",
       "25                     0.000000   25.000000  -24.000000  -24.000000  \n",
       "53                     0.000000   48.000000  -42.000000   -7.000000  \n",
       "76                     0.000000   76.000000  -76.000000        -inf  \n",
       "119                    0.000000  120.000000 -118.000000  -59.000000  \n",
       "126                    0.000000  126.000000 -126.000000        -inf  \n",
       "156                    0.000000  157.000000 -156.000000 -156.000000  \n",
       "249                    0.000000  250.000000 -249.000000 -249.000000  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = dev_small.groupby('change_amount').mean()\n",
    "tmp['profit'] = tmp.distance - tmp.new_diff\n",
    "tmp['reduction'] = 1 - tmp.new_diff / tmp.distance \n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "19ae0119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_small.distance.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8bc3d50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnd = dev_small.new_diff * (dev_small.change_amount < 5) + dev_small.distance * (dev_small.change_amount >= 5)\n",
    "cnd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c60b877a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3875968992248062"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cnd.sum() / dev_small.distance.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "50004567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distance               1.296703\n",
       "normalized_distance    0.018430\n",
       "edit_max_cldiff        0.461538\n",
       "edit_max_lendiff       0.032967\n",
       "change_amount          0.714286\n",
       "new_diff               0.747253\n",
       "dtype: float64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_small[dev_small['change_amount'] < 5].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c1133a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trash</th>\n",
       "      <th>clean</th>\n",
       "      <th>trash2</th>\n",
       "      <th>clean2</th>\n",
       "      <th>distance</th>\n",
       "      <th>normalized_distance</th>\n",
       "      <th>split</th>\n",
       "      <th>edit_max_cldiff</th>\n",
       "      <th>edit_max_lendiff</th>\n",
       "      <th>fixed</th>\n",
       "      <th>change_amount</th>\n",
       "      <th>new_diff</th>\n",
       "      <th>fixed2</th>\n",
       "      <th>fixed5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13906</th>\n",
       "      <td>Яҡшы, көр аттар ослап егеүле кырандаста йәйелеп ултырған мыйыҡһыҙ, түңәрәк ҡара һаҡаллы, оҙонсай маңлайлы кеше: - һай, оҫта тартаһың ҡурайҙы, һоҡланып бөтөрлөк түгел,- тип маҡтаны ла, сәнскеле күҙҙәре менән туп-тура ҡарап, һораны: - Ҡайһы яҡ егете һин?</td>\n",
       "      <td>Яҡшы, көр аттар ослап егеүле кырандаста йәйелеп ултырған мыйыҡһыҙ, түңәрәк ҡара һаҡаллы, оҙонсай маңлайлы кеше:      - Һай, оҫта тартаһың ҡурайҙы, һоҡланып бөтөрлөк түгел, - тип маҡтаны ла, сәнскеле күҙҙәре менән туп-тура ҡарап, һораны: - Ҡайһы яҡ егете һин?</td>\n",
       "      <td>Яҡшы, көр аттар ослап егеүле кырандаста йәйелеп ултырған мыйыҡһыҙ, түңәрәк ҡара һаҡаллы, оҙонсай маңлайлы кеше: - һай, оҫта тартаһың ҡурайҙы, һоҡланып бөтөрлөк түгел,- тип маҡтаны ла, сәнскеле күҙҙәре менән туп-тура ҡарап, һораны: - Ҡайһы яҡ егете һин?</td>\n",
       "      <td>Яҡшы, көр аттар ослап егеүле кырандаста йәйелеп ултырған мыйыҡһыҙ, түңәрәк ҡара һаҡаллы, оҙонсай маңлайлы кеше: - Һай, оҫта тартаһың ҡурайҙы, һоҡланып бөтөрлөк түгел, - тип маҡтаны ла, сәнскеле күҙҙәре менән туп-тура ҡарап, һораны: - Ҡайһы яҡ егете һин?</td>\n",
       "      <td>2</td>\n",
       "      <td>0.007905</td>\n",
       "      <td>dev</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Яҡшы, көр аттар ослап егеүле кырандаста йәйелеп ултырған мыйыҡһыҙ, түңәрәк ҡара һаҡаллы, оҙонсай маңлайлы кеше: - Ҡайһы яҡ егете һин?</td>\n",
       "      <td>119</td>\n",
       "      <td>120</td>\n",
       "      <td>Яҡшы, көр аттар ослап егеүле кырандаста йәйелеп ултырған мыйыҡһыҙ, түңәрәк ҡара һаҡаллы, оҙонсай маңлайлы кеше: - Ҡайһы яҡ егете һин?</td>\n",
       "      <td>Яҡшы, көр аттар ослап егеүле кырандаста йәйелеп ултырған мыйыҡһыҙ, түңәрәк ҡара һаҡаллы, оҙонсай маңлайлы кеше: - Ҡайһы яҡ егете һин?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14200</th>\n",
       "      <td>Ул тауҙарҙан сылтырап төшөп, унда күргәндәрен өлкән апалары Ағиҙелгә һөйләргә ашығыусы шишмәләр менән  -1.    .      йылғасыҡтар тиһеңме, ул мең төрлө сәскәләр менән ат бауырын ҡытыҡлаусы йомшаҡ үләндәр тиһеңме!</td>\n",
       "      <td>Ул тауҙарҙан сылтырап төшөп, унда күргәндәрен өлкән апалары Ағиҙелгә һөйләргә ашығыусы шишмәләр менән йылғасыҡтар тиһеңме, ул мең төрлө сәскәләр менән ат бауырын ҡытыҡлаусы йомшаҡ үләндәр тиһеңме!</td>\n",
       "      <td>Ул тауҙарҙан сылтырап төшөп, унда күргәндәрен өлкән апалары Ағиҙелгә һөйләргә ашығыусы шишмәләр менән -1. . йылғасыҡтар тиһеңме, ул мең төрлө сәскәләр менән ат бауырын ҡытыҡлаусы йомшаҡ үләндәр тиһеңме!</td>\n",
       "      <td>Ул тауҙарҙан сылтырап төшөп, унда күргәндәрен өлкән апалары Ағиҙелгә һөйләргә ашығыусы шишмәләр менән йылғасыҡтар тиһеңме, ул мең төрлө сәскәләр менән ат бауырын ҡытыҡлаусы йомшаҡ үләндәр тиһеңме!</td>\n",
       "      <td>6</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>dev</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ул тауҙарҙан сылтырап төшөп, унда күргәндәрен өлкән апалары Ағиҙелгә һөйләргә ашығыусы шишмәләр менән - ат бауырын ҡытыҡлаусы йомшаҡ үләндәр тиһеңме!</td>\n",
       "      <td>53</td>\n",
       "      <td>48</td>\n",
       "      <td>Ул тауҙарҙан сылтырап төшөп, унда күргәндәрен өлкән апалары Ағиҙелгә һөйләргә ашығыусы шишмәләр менән - ат бауырын ҡытыҡлаусы йомшаҡ үләндәр тиһеңме!</td>\n",
       "      <td>Ул тауҙарҙан сылтырап төшөп, унда күргәндәрен өлкән апалары Ағиҙелгә һөйләргә ашығыусы шишмәләр менән - ат бауырын ҡытыҡлаусы йомшаҡ үләндәр тиһеңме!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12319</th>\n",
       "      <td>Алдашып, кемделер бәләнән ҡурсалап ҡала ине. ...Ә шулай ҙа йәшерәк сағында бер мәл уны шайтан ҡоторта яҙып, ҡоторта яҙып ҡалды.</td>\n",
       "      <td>Алдашып, кемделер бәләнән ҡурсалап ҡала ине.      ...Ә шулай ҙа йәшерәк сағында бер мәл уны шайтан ҡоторта яҙып, ҡоторта яҙып ҡалды.</td>\n",
       "      <td>Алдашып, кемделер бәләнән ҡурсалап ҡала ине. ...Ә шулай ҙа йәшерәк сағында бер мәл уны шайтан ҡоторта яҙып, ҡоторта яҙып ҡалды.</td>\n",
       "      <td>Алдашып, кемделер бәләнән ҡурсалап ҡала ине. ...Ә шулай ҙа йәшерәк сағында бер мәл уны шайтан ҡоторта яҙып, ҡоторта яҙып ҡалды.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>dev</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Алдашып, кемделер бәләнән ҡурсалап ҡала ине....Ә шулай ҙа йәшерәк сағында бер мәл уны шайтан ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>Алдашып, кемделер бәләнән ҡурсалап ҡала ине....Ә шулай ҙа йәшерәк сағында бер мәл уны шайтан ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып ҡалды.</td>\n",
       "      <td>Алдашып, кемделер бәләнән ҡурсалап ҡала ине....Ә шулай ҙа йәшерәк сағында бер мәл уны шайтан ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып ҡалды.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12851</th>\n",
       "      <td>Китмәгеҙ, ҡошҡайҙар, китмәгеҙ, Күңелде һагышлы итмәгеҙ!</td>\n",
       "      <td>Китмәгеҙ, ҡошҡайҙар, китмәгеҙ,      Күңелде һағышлы итмәгеҙ!</td>\n",
       "      <td>Китмәгеҙ, ҡошҡайҙар, китмәгеҙ, Күңелде һагышлы итмәгеҙ!</td>\n",
       "      <td>Китмәгеҙ, ҡошҡайҙар, китмәгеҙ, Күңелде һағышлы итмәгеҙ!</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>dev</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Китмәгеҙ, ҡошҡайҙар, китмәгеҙ, ҡошҡайҙар, китмәгеҙ, ҡошҡайҙар, китмәгеҙ, ҡошҡайҙар, китмәгеҙ, ҡошҡайҙар, китмәгеҙ, ҡошҡайҙар, китмәгеҙ, ҡошҡайҙар, китмәгеҙ, ҡошҡайҙар, китмәгеҙ!</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>Китмәгеҙ, ҡошҡайҙар, китмәгеҙ!</td>\n",
       "      <td>Китмәгеҙ, ҡошҡайҙар, китмәгеҙ!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23336</th>\n",
       "      <td>Үҙе шундай ғына булһа ла, ҡулында - эш, телендә ут уйнатты, әммә алағайымға ләстит һатып та йөрөмәне ул. Һуғыштың урталарында ауылға ҙур ғына бер төркөм һалдат килгән.</td>\n",
       "      <td>Үҙе шундай ғына булһа ла, ҡулында - эш, телендә ут уйнатты, әммә алағайымға ләстит һатып та йөрөмәне ул.      Һуғыштың урталарында ауылға ҙур ғына бер төркөм һалдат килгән.</td>\n",
       "      <td>Үҙе шундай ғына булһа ла, ҡулында - эш, телендә ут уйнатты, әммә алағайымға ләстит һатып та йөрөмәне ул. Һуғыштың урталарында ауылға ҙур ғына бер төркөм һалдат килгән.</td>\n",
       "      <td>Үҙе шундай ғына булһа ла, ҡулында - эш, телендә ут уйнатты, әммә алағайымға ләстит һатып та йөрөмәне ул. Һуғыштың урталарында ауылға ҙур ғына бер төркөм һалдат килгән.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>dev</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Үҙе шундай ғына бер төркөм һалдат килгән.</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>Үҙе шундай ғына бер төркөм һалдат килгән.</td>\n",
       "      <td>Үҙе шундай ғына бер төркөм һалдат килгән.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16461</th>\n",
       "      <td>Аҡ Йорт ҡуяны өс тапҡыр борғо уйнатты, төрөлгән пергамент ҡағыҙҙы тағатып ебәрҙе лә уҡый башланы: Ханбикәбеҙ күп бауырһаҡ бешергән, Бауырһаҡты балға-майға төшөргән.</td>\n",
       "      <td>Аҡ Йорт ҡуяны өс тапҡыр борғо уйнатты, төрөлгән пергамент ҡағыҙҙы тағатып ебәрҙе лә уҡый башланы:      Ханбикәбеҙ күп бауырһаҡ бешергән,      Бауырһаҡты балға-майға төшөргән.</td>\n",
       "      <td>Аҡ Йорт ҡуяны өс тапҡыр борғо уйнатты, төрөлгән пергамент ҡағыҙҙы тағатып ебәрҙе лә уҡый башланы: Ханбикәбеҙ күп бауырһаҡ бешергән, Бауырһаҡты балға-майға төшөргән.</td>\n",
       "      <td>Аҡ Йорт ҡуяны өс тапҡыр борғо уйнатты, төрөлгән пергамент ҡағыҙҙы тағатып ебәрҙе лә уҡый башланы: Ханбикәбеҙ күп бауырһаҡ бешергән, Бауырһаҡты балға-майға төшөргән.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>dev</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Аҡ Йорт ҡуяны өс тапҡыр борғо уйнатты, төрөлгән пергамент ҡағыҙҙы тағатып ебәрҙе лә уҡый башланы: Ханбикәбеҙ күп бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ беш...</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>Аҡ Йорт ҡуяны өс тапҡыр борғо уйнатты, төрөлгән пергамент ҡағыҙҙы тағатып ебәрҙе лә уҡый башланы: Ханбикәбеҙ күп бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡты балға-майға төшөргән.</td>\n",
       "      <td>Аҡ Йорт ҡуяны өс тапҡыр борғо уйнатты, төрөлгән пергамент ҡағыҙҙы тағатып ебәрҙе лә уҡый башланы: Ханбикәбеҙ күп бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡты балға-майға төшөргән.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8938</th>\n",
       "      <td>Хәҙер белә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә бара.</td>\n",
       "      <td>Хәҙер белә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, көлгә әүерелә бара.</td>\n",
       "      <td>Хәҙер белә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә бара.</td>\n",
       "      <td>Хәҙер белә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, көлгә әүерелә бара.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>dev</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Хәҙер белә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һа...</td>\n",
       "      <td>249</td>\n",
       "      <td>250</td>\n",
       "      <td>Хәҙер белә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һа...</td>\n",
       "      <td>Хәҙер белә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3835</th>\n",
       "      <td>Бышылдап ҡына өс тапҡыр: «Крекс, феке, пекс», - тине, дүрт алтын аҡсаһын соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡор өҫтөнә һипте.</td>\n",
       "      <td>Бышылдап ҡына өс тапҡыр: «Крекс, фекс, пекс», - тине, дүрт алтын аҡсаһын соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡор өҫтөнә һипте.</td>\n",
       "      <td>Бышылдап ҡына өс тапҡыр: «Крекс, феке, пекс», - тине, дүрт алтын аҡсаһын соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡор өҫтөнә һипте.</td>\n",
       "      <td>Бышылдап ҡына өс тапҡыр: «Крекс, фекс, пекс», - тине, дүрт алтын аҡсаһын соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡор өҫтөнә һипте.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>dev</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Бышылдап ҡына өс тапҡыр: «Крекс, феке, пекс», - тине, дүрт алтын аҡсаһын соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып к...</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>Бышылдап ҡына өс тапҡыр: «Крекс, феке, пекс», - тине, дүрт алтын аҡсаһын соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып к...</td>\n",
       "      <td>Бышылдап ҡына өс тапҡыр: «Крекс, феке, пекс», - тине, дүрт алтын аҡсаһын соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡор өҫтөнә һи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22768</th>\n",
       "      <td>Утрауҙа кеше бар, ул төн уртаһында миңә ҡысҡыра: - Робин, Робин, Робин Крузо!</td>\n",
       "      <td>Утрауҙа кеше бар, ул төн уртаһында миңә ҡысҡыра:      - Робин, Робин, Робин Крузо!</td>\n",
       "      <td>Утрауҙа кеше бар, ул төн уртаһында миңә ҡысҡыра: - Робин, Робин, Робин Крузо!</td>\n",
       "      <td>Утрауҙа кеше бар, ул төн уртаһында миңә ҡысҡыра: - Робин, Робин, Робин Крузо!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>dev</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Утрауҙа кеше бар, ул төн уртаһында миңә ҡысҡыра: - Робин, Робин, Робин, Робин Крузо!</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Утрауҙа кеше бар, ул төн уртаһында миңә ҡысҡыра: - Робин, Робин Крузо!</td>\n",
       "      <td>Утрауҙа кеше бар, ул төн уртаһында миңә ҡысҡыра: - Робин, Робин Крузо!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                              trash  \\\n",
       "13906  Яҡшы, көр аттар ослап егеүле кырандаста йәйелеп ултырған мыйыҡһыҙ, түңәрәк ҡара һаҡаллы, оҙонсай маңлайлы кеше: - һай, оҫта тартаһың ҡурайҙы, һоҡланып бөтөрлөк түгел,- тип маҡтаны ла, сәнскеле күҙҙәре менән туп-тура ҡарап, һораны: - Ҡайһы яҡ егете һин?   \n",
       "14200                                           Ул тауҙарҙан сылтырап төшөп, унда күргәндәрен өлкән апалары Ағиҙелгә һөйләргә ашығыусы шишмәләр менән  -1.    .      йылғасыҡтар тиһеңме, ул мең төрлө сәскәләр менән ат бауырын ҡытыҡлаусы йомшаҡ үләндәр тиһеңме!   \n",
       "12319                                                                                                                               Алдашып, кемделер бәләнән ҡурсалап ҡала ине. ...Ә шулай ҙа йәшерәк сағында бер мәл уны шайтан ҡоторта яҙып, ҡоторта яҙып ҡалды.   \n",
       "12851                                                                                                                                                                                                       Китмәгеҙ, ҡошҡайҙар, китмәгеҙ, Күңелде һагышлы итмәгеҙ!   \n",
       "23336                                                                                       Үҙе шундай ғына булһа ла, ҡулында - эш, телендә ут уйнатты, әммә алағайымға ләстит һатып та йөрөмәне ул. Һуғыштың урталарында ауылға ҙур ғына бер төркөм һалдат килгән.   \n",
       "16461                                                                                          Аҡ Йорт ҡуяны өс тапҡыр борғо уйнатты, төрөлгән пергамент ҡағыҙҙы тағатып ебәрҙе лә уҡый башланы: Ханбикәбеҙ күп бауырһаҡ бешергән, Бауырһаҡты балға-майға төшөргән.   \n",
       "8938                                                                                                                                                            Хәҙер белә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә бара.   \n",
       "3835                                                                                                               Бышылдап ҡына өс тапҡыр: «Крекс, феке, пекс», - тине, дүрт алтын аҡсаһын соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡор өҫтөнә һипте.   \n",
       "22768                                                                                                                                                                                 Утрауҙа кеше бар, ул төн уртаһында миңә ҡысҡыра: - Робин, Робин, Робин Крузо!   \n",
       "\n",
       "                                                                                                                                                                                                                                                                    clean  \\\n",
       "13906  Яҡшы, көр аттар ослап егеүле кырандаста йәйелеп ултырған мыйыҡһыҙ, түңәрәк ҡара һаҡаллы, оҙонсай маңлайлы кеше:      - Һай, оҫта тартаһың ҡурайҙы, һоҡланып бөтөрлөк түгел, - тип маҡтаны ла, сәнскеле күҙҙәре менән туп-тура ҡарап, һораны: - Ҡайһы яҡ егете һин?   \n",
       "14200                                                                Ул тауҙарҙан сылтырап төшөп, унда күргәндәрен өлкән апалары Ағиҙелгә һөйләргә ашығыусы шишмәләр менән йылғасыҡтар тиһеңме, ул мең төрлө сәскәләр менән ат бауырын ҡытыҡлаусы йомшаҡ үләндәр тиһеңме!   \n",
       "12319                                                                                                                                Алдашып, кемделер бәләнән ҡурсалап ҡала ине.      ...Ә шулай ҙа йәшерәк сағында бер мәл уны шайтан ҡоторта яҙып, ҡоторта яҙып ҡалды.   \n",
       "12851                                                                                                                                                                                                        Китмәгеҙ, ҡошҡайҙар, китмәгеҙ,      Күңелде һағышлы итмәгеҙ!   \n",
       "23336                                                                                        Үҙе шундай ғына булһа ла, ҡулында - эш, телендә ут уйнатты, әммә алағайымға ләстит һатып та йөрөмәне ул.      Һуғыштың урталарында ауылға ҙур ғына бер төркөм һалдат килгән.   \n",
       "16461                                                                                      Аҡ Йорт ҡуяны өс тапҡыр борғо уйнатты, төрөлгән пергамент ҡағыҙҙы тағатып ебәрҙе лә уҡый башланы:      Ханбикәбеҙ күп бауырһаҡ бешергән,      Бауырһаҡты балға-майға төшөргән.   \n",
       "8938                                                                                                                                                                  Хәҙер белә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, көлгә әүерелә бара.   \n",
       "3835                                                                                                                     Бышылдап ҡына өс тапҡыр: «Крекс, фекс, пекс», - тине, дүрт алтын аҡсаһын соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡор өҫтөнә һипте.   \n",
       "22768                                                                                                                                                                                  Утрауҙа кеше бар, ул төн уртаһында миңә ҡысҡыра:      - Робин, Робин, Робин Крузо!   \n",
       "\n",
       "                                                                                                                                                                                                                                                             trash2  \\\n",
       "13906  Яҡшы, көр аттар ослап егеүле кырандаста йәйелеп ултырған мыйыҡһыҙ, түңәрәк ҡара һаҡаллы, оҙонсай маңлайлы кеше: - һай, оҫта тартаһың ҡурайҙы, һоҡланып бөтөрлөк түгел,- тип маҡтаны ла, сәнскеле күҙҙәре менән туп-тура ҡарап, һораны: - Ҡайһы яҡ егете һин?   \n",
       "14200                                                    Ул тауҙарҙан сылтырап төшөп, унда күргәндәрен өлкән апалары Ағиҙелгә һөйләргә ашығыусы шишмәләр менән -1. . йылғасыҡтар тиһеңме, ул мең төрлө сәскәләр менән ат бауырын ҡытыҡлаусы йомшаҡ үләндәр тиһеңме!   \n",
       "12319                                                                                                                               Алдашып, кемделер бәләнән ҡурсалап ҡала ине. ...Ә шулай ҙа йәшерәк сағында бер мәл уны шайтан ҡоторта яҙып, ҡоторта яҙып ҡалды.   \n",
       "12851                                                                                                                                                                                                       Китмәгеҙ, ҡошҡайҙар, китмәгеҙ, Күңелде һагышлы итмәгеҙ!   \n",
       "23336                                                                                       Үҙе шундай ғына булһа ла, ҡулында - эш, телендә ут уйнатты, әммә алағайымға ләстит һатып та йөрөмәне ул. Һуғыштың урталарында ауылға ҙур ғына бер төркөм һалдат килгән.   \n",
       "16461                                                                                          Аҡ Йорт ҡуяны өс тапҡыр борғо уйнатты, төрөлгән пергамент ҡағыҙҙы тағатып ебәрҙе лә уҡый башланы: Ханбикәбеҙ күп бауырһаҡ бешергән, Бауырһаҡты балға-майға төшөргән.   \n",
       "8938                                                                                                                                                            Хәҙер белә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә бара.   \n",
       "3835                                                                                                               Бышылдап ҡына өс тапҡыр: «Крекс, феке, пекс», - тине, дүрт алтын аҡсаһын соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡор өҫтөнә һипте.   \n",
       "22768                                                                                                                                                                                 Утрауҙа кеше бар, ул төн уртаһында миңә ҡысҡыра: - Робин, Робин, Робин Крузо!   \n",
       "\n",
       "                                                                                                                                                                                                                                                              clean2  \\\n",
       "13906  Яҡшы, көр аттар ослап егеүле кырандаста йәйелеп ултырған мыйыҡһыҙ, түңәрәк ҡара һаҡаллы, оҙонсай маңлайлы кеше: - Һай, оҫта тартаһың ҡурайҙы, һоҡланып бөтөрлөк түгел, - тип маҡтаны ла, сәнскеле күҙҙәре менән туп-тура ҡарап, һораны: - Ҡайһы яҡ егете һин?   \n",
       "14200                                                           Ул тауҙарҙан сылтырап төшөп, унда күргәндәрен өлкән апалары Ағиҙелгә һөйләргә ашығыусы шишмәләр менән йылғасыҡтар тиһеңме, ул мең төрлө сәскәләр менән ат бауырын ҡытыҡлаусы йомшаҡ үләндәр тиһеңме!   \n",
       "12319                                                                                                                                Алдашып, кемделер бәләнән ҡурсалап ҡала ине. ...Ә шулай ҙа йәшерәк сағында бер мәл уны шайтан ҡоторта яҙып, ҡоторта яҙып ҡалды.   \n",
       "12851                                                                                                                                                                                                        Китмәгеҙ, ҡошҡайҙар, китмәгеҙ, Күңелде һағышлы итмәгеҙ!   \n",
       "23336                                                                                        Үҙе шундай ғына булһа ла, ҡулында - эш, телендә ут уйнатты, әммә алағайымға ләстит һатып та йөрөмәне ул. Һуғыштың урталарында ауылға ҙур ғына бер төркөм һалдат килгән.   \n",
       "16461                                                                                           Аҡ Йорт ҡуяны өс тапҡыр борғо уйнатты, төрөлгән пергамент ҡағыҙҙы тағатып ебәрҙе лә уҡый башланы: Ханбикәбеҙ күп бауырһаҡ бешергән, Бауырһаҡты балға-майға төшөргән.   \n",
       "8938                                                                                                                                                             Хәҙер белә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, көлгә әүерелә бара.   \n",
       "3835                                                                                                                Бышылдап ҡына өс тапҡыр: «Крекс, фекс, пекс», - тине, дүрт алтын аҡсаһын соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡор өҫтөнә һипте.   \n",
       "22768                                                                                                                                                                                  Утрауҙа кеше бар, ул төн уртаһында миңә ҡысҡыра: - Робин, Робин, Робин Крузо!   \n",
       "\n",
       "       distance  normalized_distance split  edit_max_cldiff  edit_max_lendiff  \\\n",
       "13906         2             0.007905   dev                1                 0   \n",
       "14200         6             0.029703   dev                0                 0   \n",
       "12319         0             0.000000   dev                0                 0   \n",
       "12851         1             0.018182   dev                1                 0   \n",
       "23336         0             0.000000   dev                0                 0   \n",
       "16461         0             0.000000   dev                0                 0   \n",
       "8938          1             0.010101   dev                1                 0   \n",
       "3835          1             0.006944   dev                1                 0   \n",
       "22768         0             0.000000   dev                0                 0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                             fixed  \\\n",
       "13906                                                                                                                                                                        Яҡшы, көр аттар ослап егеүле кырандаста йәйелеп ултырған мыйыҡһыҙ, түңәрәк ҡара һаҡаллы, оҙонсай маңлайлы кеше: - Ҡайһы яҡ егете һин?   \n",
       "14200                                                                                                                                                        Ул тауҙарҙан сылтырап төшөп, унда күргәндәрен өлкән апалары Ағиҙелгә һөйләргә ашығыусы шишмәләр менән - ат бауырын ҡытыҡлаусы йомшаҡ үләндәр тиһеңме!   \n",
       "12319  Алдашып, кемделер бәләнән ҡурсалап ҡала ине....Ә шулай ҙа йәшерәк сағында бер мәл уны шайтан ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта...   \n",
       "12851                                                                                                                            Китмәгеҙ, ҡошҡайҙар, китмәгеҙ, ҡошҡайҙар, китмәгеҙ, ҡошҡайҙар, китмәгеҙ, ҡошҡайҙар, китмәгеҙ, ҡошҡайҙар, китмәгеҙ, ҡошҡайҙар, китмәгеҙ, ҡошҡайҙар, китмәгеҙ, ҡошҡайҙар, китмәгеҙ!   \n",
       "23336                                                                                                                                                                                                                                                                    Үҙе шундай ғына бер төркөм һалдат килгән.   \n",
       "16461  Аҡ Йорт ҡуяны өс тапҡыр борғо уйнатты, төрөлгән пергамент ҡағыҙҙы тағатып ебәрҙе лә уҡый башланы: Ханбикәбеҙ күп бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ беш...   \n",
       "8938   Хәҙер белә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һа...   \n",
       "3835   Бышылдап ҡына өс тапҡыр: «Крекс, феке, пекс», - тине, дүрт алтын аҡсаһын соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып к...   \n",
       "22768                                                                                                                                                                                                                         Утрауҙа кеше бар, ул төн уртаһында миңә ҡысҡыра: - Робин, Робин, Робин, Робин Крузо!   \n",
       "\n",
       "       change_amount  new_diff  \\\n",
       "13906            119       120   \n",
       "14200             53        48   \n",
       "12319             15        15   \n",
       "12851             25        25   \n",
       "23336            126       126   \n",
       "16461             76        76   \n",
       "8938             249       250   \n",
       "3835             156       157   \n",
       "22768              7         7   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                            fixed2  \\\n",
       "13906                                                                                                                                                                        Яҡшы, көр аттар ослап егеүле кырандаста йәйелеп ултырған мыйыҡһыҙ, түңәрәк ҡара һаҡаллы, оҙонсай маңлайлы кеше: - Ҡайһы яҡ егете һин?   \n",
       "14200                                                                                                                                                        Ул тауҙарҙан сылтырап төшөп, унда күргәндәрен өлкән апалары Ағиҙелгә һөйләргә ашығыусы шишмәләр менән - ат бауырын ҡытыҡлаусы йомшаҡ үләндәр тиһеңме!   \n",
       "12319                                                                                                                                     Алдашып, кемделер бәләнән ҡурсалап ҡала ине....Ә шулай ҙа йәшерәк сағында бер мәл уны шайтан ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып ҡалды.   \n",
       "12851                                                                                                                                                                                                                                                                               Китмәгеҙ, ҡошҡайҙар, китмәгеҙ!   \n",
       "23336                                                                                                                                                                                                                                                                    Үҙе шундай ғына бер төркөм һалдат килгән.   \n",
       "16461                                                             Аҡ Йорт ҡуяны өс тапҡыр борғо уйнатты, төрөлгән пергамент ҡағыҙҙы тағатып ебәрҙе лә уҡый башланы: Ханбикәбеҙ күп бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡты балға-майға төшөргән.   \n",
       "8938   Хәҙер белә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һа...   \n",
       "3835   Бышылдап ҡына өс тапҡыр: «Крекс, феке, пекс», - тине, дүрт алтын аҡсаһын соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып к...   \n",
       "22768                                                                                                                                                                                                                                       Утрауҙа кеше бар, ул төн уртаһында миңә ҡысҡыра: - Робин, Робин Крузо!   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                            fixed5  \n",
       "13906                                                                                                                                                                        Яҡшы, көр аттар ослап егеүле кырандаста йәйелеп ултырған мыйыҡһыҙ, түңәрәк ҡара һаҡаллы, оҙонсай маңлайлы кеше: - Ҡайһы яҡ егете һин?  \n",
       "14200                                                                                                                                                        Ул тауҙарҙан сылтырап төшөп, унда күргәндәрен өлкән апалары Ағиҙелгә һөйләргә ашығыусы шишмәләр менән - ат бауырын ҡытыҡлаусы йомшаҡ үләндәр тиһеңме!  \n",
       "12319                                                                                                                                                                 Алдашып, кемделер бәләнән ҡурсалап ҡала ине....Ә шулай ҙа йәшерәк сағында бер мәл уны шайтан ҡоторта яҙып, ҡоторта яҙып, ҡоторта яҙып ҡалды.  \n",
       "12851                                                                                                                                                                                                                                                                               Китмәгеҙ, ҡошҡайҙар, китмәгеҙ!  \n",
       "23336                                                                                                                                                                                                                                                                    Үҙе шундай ғына бер төркөм һалдат килгән.  \n",
       "16461                                                             Аҡ Йорт ҡуяны өс тапҡыр борғо уйнатты, төрөлгән пергамент ҡағыҙҙы тағатып ебәрҙе лә уҡый башланы: Ханбикәбеҙ күп бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡ бешергән, Бауырһаҡты балға-майға төшөргән.  \n",
       "8938   Хәҙер белә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һауала тулыһынса күмергә әйләнмәй, кәлгә әүерелә: шулай итмәһәң, янған әрҙәнә асыҡ һа...  \n",
       "3835   Бышылдап ҡына өс тапҡыр: «Крекс, феке, пекс», - тине, дүрт алтын аҡсаһын соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡорға һалып күмде, кеҫәһенән бер семтем тоҙ алып, соҡор өҫтөнә һи...  \n",
       "22768                                                                                                                                                                                                                                       Утрауҙа кеше бар, ул төн уртаһында миңә ҡысҡыра: - Робин, Робин Крузо!  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_small[dev_small.change_amount > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4af35e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../models/t5-tiny-denoise-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5a1af952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/t5-tiny-denoise-v1\\\\tokenizer_config.json',\n",
       " '../models/t5-tiny-denoise-v1\\\\special_tokens_map.json',\n",
       " '../models/t5-tiny-denoise-v1\\\\added_tokens.json')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(path)\n",
    "tokenizer.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6ae21",
   "metadata": {},
   "source": [
    "# Now try adding synthetic noise to the clean training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "0f9889ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def findall(haystack, needle):\n",
    "    first_ids = []\n",
    "    i = haystack.find(needle)\n",
    "    while i != -1:\n",
    "        first_ids.append(i)\n",
    "        i = haystack.find(needle, i+1)\n",
    "    return first_ids\n",
    "\n",
    "\n",
    "class Noiser():\n",
    "    def __init__(self, deletions_cnt=None, new2olds=None, edit_total_proba=None, char_prior=None):\n",
    "        self.deletions_cnt = deletions_cnt\n",
    "        self.new2olds = new2olds\n",
    "        self.edit_total_proba = edit_total_proba\n",
    "        self.char_prior = char_prior\n",
    "    \n",
    "    def save(self, path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(\n",
    "                {\n",
    "                    'deletions_cnt': self.deletions_cnt, \n",
    "                    'new2olds': self.new2olds, \n",
    "                    'edit_total_proba': self.edit_total_proba, \n",
    "                    'char_prior': self.char_prior,\n",
    "                }, \n",
    "                f, \n",
    "                ensure_ascii=False, indent=1\n",
    "            )\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return cls(\n",
    "            deletions_cnt=data['deletions_cnt'],\n",
    "            new2olds=data['new2olds'],\n",
    "            edit_total_proba=data['edit_total_proba'],\n",
    "            char_prior=data['char_prior'],\n",
    "        )\n",
    "    \n",
    "    def add_noise(\n",
    "        self, text, min_edits=1, max_edits=10, p_exit=None, p_insert=0.18, temp=0.3, p_randchar=0.05, edit_rate=0.02,\n",
    "        eps=1e-10,\n",
    "    ):\n",
    "        # todo: smooth all probabilities\n",
    "        if p_exit is None:\n",
    "            p_exit = 1 / (max(2, len(text)) * edit_rate)\n",
    "        # replacing a random character (including deletions)\n",
    "        candidates = [text[i:i+n] for n in range(1, 4) for i in range(len(text)-n+1)]\n",
    "        cand_weights = [self.edit_total_proba.get(c, 0) ** temp + eps for c in candidates]\n",
    "        n_edits = 0\n",
    "        for i in range(max_edits):\n",
    "            if random.random() < p_randchar:\n",
    "                rep = random.choices(\n",
    "                    list(self.char_prior.keys()), weights=[c + eps for c in self.char_prior.values()]\n",
    "                )[0]\n",
    "                idx = random.randint(0, len(text)-1)\n",
    "                text = text[:idx] + rep + text[idx + 1:]\n",
    "            elif random.random() < p_insert:\n",
    "                idx = random.randint(0, len(text))\n",
    "                insertion = random.choices(\n",
    "                    list(self.deletions_cnt.keys()), \n",
    "                    weights=[c + eps for c in self.deletions_cnt.values()]\n",
    "                )[0]\n",
    "                text = text[:idx] + insertion +  text[idx:]\n",
    "                n_edits += 1\n",
    "            else:\n",
    "                choice = random.choices(candidates, weights=cand_weights)[0]\n",
    "                if choice not in text:\n",
    "                    continue\n",
    "                idx = random.choice(findall(text, choice))\n",
    "                rd = self.new2olds.get(choice, self.char_prior)\n",
    "                replacement = random.choices(\n",
    "                    list(rd.keys()), \n",
    "                    weights=[c + eps for c in rd.values()]\n",
    "                )[0]\n",
    "                text = text[:idx] + replacement +  text[idx + len(choice):]\n",
    "                n_edits += 1\n",
    "            if n_edits >= min_edits and random.random() < p_exit:\n",
    "                break\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f0ebf748",
   "metadata": {},
   "outputs": [],
   "source": [
    "noiser = Noiser.load('noise_model_v1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "49a912be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = random.choice(clean_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "aac40965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тәбиғәтте һаҡлау буйынса сараларҙа ҡатнашыусыларҙың йылдан-йыл артыуы ла шатландырмай ҡалмай.\n",
      "Тәбиғәтте һаҡлау буйынса сараларҙа ҡстнашыусыларҙың йылдан-йыл артыуы ла шатләнән әрмай ҡд д май.\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print(noiser.add_noise(text, edit_rate=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b6dc8eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4d745f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "share_real = 0.1\n",
    "p_keep = 0.5\n",
    "grad_steps = 8\n",
    "report_steps = 1000\n",
    "bs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "54d653e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "f909c2e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241e5c62a7ac46a490cb13bba833914c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 0.1472825689483434\n",
      "step 1000 loss 0.14194738892652095\n",
      "step 2000 loss 0.1323908112803474\n",
      "error 2754 CUDA out of memory. Tried to allocate 88.00 MiB (GPU 0; 4.00 GiB total capacity; 2.51 GiB already allocated; 6.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 3000 loss 0.148925592886284\n",
      "step 4000 loss 0.13794131877133622\n",
      "error 4802 CUDA out of memory. Tried to allocate 218.00 MiB (GPU 0; 4.00 GiB total capacity; 2.31 GiB already allocated; 128.20 MiB free; 2.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 5000 loss 0.14397644076682628\n",
      "step 6000 loss 0.1426956572830677\n",
      "step 7000 loss 0.14415900671854615\n",
      "step 8000 loss 0.141122937197797\n",
      "step 9000 loss 0.1394832178177312\n",
      "step 10000 loss 0.14356087400298564\n",
      "step 11000 loss 0.14157445475645364\n",
      "step 12000 loss 0.13786635510530323\n",
      "error 12254 CUDA out of memory. Tried to allocate 146.00 MiB (GPU 0; 4.00 GiB total capacity; 2.40 GiB already allocated; 96.20 MiB free; 2.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 13000 loss 0.14736592090129852\n",
      "step 14000 loss 0.14399459447991103\n",
      "step 15000 loss 0.14985729610547424\n",
      "step 16000 loss 0.13610939750261605\n",
      "step 17000 loss 0.1419573279581964\n",
      "error 17582 CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 4.00 GiB total capacity; 2.57 GiB already allocated; 2.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 18000 loss 0.14255173535645008\n",
      "step 19000 loss 0.14496894742827862\n",
      "step 20000 loss 0.14559041133243592\n",
      "step 21000 loss 0.13905582084972412\n",
      "step 22000 loss 0.14377444547228516\n",
      "step 23000 loss 0.13413883091975004\n",
      "step 24000 loss 0.1367825522776693\n",
      "step 25000 loss 0.14065516957920043\n",
      "step 26000 loss 0.1485273240460083\n",
      "step 27000 loss 0.14424725520797074\n",
      "error 27548 CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 4.00 GiB total capacity; 2.58 GiB already allocated; 8.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 28000 loss 0.14573299415735527\n",
      "error 28527 CUDA out of memory. Tried to allocate 200.00 MiB (GPU 0; 4.00 GiB total capacity; 2.38 GiB already allocated; 146.20 MiB free; 2.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 29000 loss 0.14298387544043362\n",
      "error 29540 CUDA out of memory. Tried to allocate 372.00 MiB (GPU 0; 4.00 GiB total capacity; 2.38 GiB already allocated; 258.20 MiB free; 2.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 30000 loss 0.14296222609467804\n",
      "step 31000 loss 0.14779071442969144\n",
      "step 32000 loss 0.13631158122885972\n",
      "error 32497 CUDA out of memory. Tried to allocate 614.00 MiB (GPU 0; 4.00 GiB total capacity; 2.30 GiB already allocated; 270.20 MiB free; 2.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 33000 loss 0.141860095590353\n",
      "error 33788 CUDA out of memory. Tried to allocate 4.38 GiB (GPU 0; 4.00 GiB total capacity; 244.96 MiB already allocated; 2.26 GiB free; 392.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 34000 loss 0.13745756815932691\n",
      "step 35000 loss 0.14348018508497626\n",
      "step 36000 loss 0.13242995547130704\n",
      "error 36943 CUDA out of memory. Tried to allocate 248.00 MiB (GPU 0; 4.00 GiB total capacity; 2.28 GiB already allocated; 186.20 MiB free; 2.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 37000 loss 0.12825718821864576\n",
      "step 38000 loss 0.13297711108624935\n",
      "step 39000 loss 0.13668920855782926\n",
      "error 39813 CUDA out of memory. Tried to allocate 146.00 MiB (GPU 0; 4.00 GiB total capacity; 2.01 GiB already allocated; 40.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 40000 loss 0.13589663291256876\n",
      "step 41000 loss 0.14103582141455262\n",
      "step 42000 loss 0.14305322782881558\n",
      "step 43000 loss 0.14737459028791636\n",
      "step 44000 loss 0.14815653117094188\n",
      "step 45000 loss 0.12989896611589938\n",
      "step 46000 loss 0.1423274344848469\n",
      "step 47000 loss 0.14475929201673715\n",
      "error 47083 CUDA out of memory. Tried to allocate 110.00 MiB (GPU 0; 4.00 GiB total capacity; 2.52 GiB already allocated; 52.20 MiB free; 2.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 48000 loss 0.14084342170413583\n",
      "step 49000 loss 0.1345302871251479\n",
      "step 50000 loss 0.13616972717363388\n",
      "step 51000 loss 0.14275679941568523\n",
      "step 52000 loss 0.13895785355288534\n",
      "error 52617 CUDA out of memory. Tried to allocate 156.00 MiB (GPU 0; 4.00 GiB total capacity; 2.56 GiB already allocated; 60.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 52694 CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 96.20 MiB free; 2.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 53000 loss 0.13879758257977665\n",
      "step 54000 loss 0.13597760116122662\n",
      "step 55000 loss 0.13737189428135752\n",
      "step 56000 loss 0.14553682917077093\n",
      "step 57000 loss 0.13642754750698805\n",
      "step 58000 loss 0.1358441714681685\n",
      "step 59000 loss 0.14397451534029096\n",
      "step 60000 loss 0.14251536749396473\n",
      "step 61000 loss 0.1394966616667807\n",
      "step 62000 loss 0.13175074171926826\n",
      "step 63000 loss 0.13444992551673204\n",
      "error 63126 CUDA out of memory. Tried to allocate 450.00 MiB (GPU 0; 4.00 GiB total capacity; 1.84 GiB already allocated; 428.20 MiB free; 2.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 64000 loss 0.14528458316717296\n",
      "step 65000 loss 0.13727787282783538\n",
      "error 65291 CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 4.00 GiB total capacity; 2.55 GiB already allocated; 54.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 66000 loss 0.1333738476773724\n",
      "step 67000 loss 0.13223497564718129\n",
      "error 67585 CUDA out of memory. Tried to allocate 182.00 MiB (GPU 0; 4.00 GiB total capacity; 2.50 GiB already allocated; 96.20 MiB free; 2.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 68000 loss 0.13015899854991586\n",
      "step 69000 loss 0.14279593732859938\n",
      "step 70000 loss 0.14579197085415946\n",
      "step 71000 loss 0.13612105597741903\n",
      "step 72000 loss 0.1312505107978359\n",
      "step 73000 loss 0.1382298058597371\n",
      "step 74000 loss 0.13328957142867148\n",
      "step 75000 loss 0.1402048734482378\n",
      "step 76000 loss 0.12217005340848118\n",
      "step 77000 loss 0.1405895093381405\n",
      "step 78000 loss 0.12890467978268863\n",
      "step 79000 loss 0.13041056259814648\n",
      "step 80000 loss 0.14282028002664446\n",
      "step 81000 loss 0.13665017681475727\n",
      "step 82000 loss 0.12154696011450142\n",
      "step 83000 loss 0.1329771571904421\n",
      "step 84000 loss 0.13211077723139897\n",
      "step 85000 loss 0.13998162765055894\n",
      "step 86000 loss 0.13383974360488354\n",
      "error 86992 CUDA out of memory. Tried to allocate 534.00 MiB (GPU 0; 4.00 GiB total capacity; 2.34 GiB already allocated; 252.20 MiB free; 2.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 87000 loss 0.14324370968248695\n",
      "step 88000 loss 0.14490564293414354\n",
      "step 89000 loss 0.12999817048665135\n",
      "step 90000 loss 0.1374334414852783\n",
      "step 91000 loss 0.13000853368174284\n",
      "step 92000 loss 0.12326931228023022\n",
      "step 93000 loss 0.13803775489516557\n",
      "error 93857 CUDA out of memory. Tried to allocate 536.00 MiB (GPU 0; 4.00 GiB total capacity; 1.37 GiB already allocated; 392.20 MiB free; 2.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 94000 loss 0.13331184531515464\n",
      "error 94961 CUDA out of memory. Tried to allocate 296.00 MiB (GPU 0; 4.00 GiB total capacity; 2.31 GiB already allocated; 92.20 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 95000 loss 0.13512308008503168\n",
      "step 96000 loss 0.13040408732555805\n",
      "step 97000 loss 0.14658688639104367\n",
      "error 97798 CUDA out of memory. Tried to allocate 494.00 MiB (GPU 0; 4.00 GiB total capacity; 2.01 GiB already allocated; 242.20 MiB free; 2.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 98000 loss 0.12806914102751762\n",
      "step 99000 loss 0.13779114238917828\n",
      "step 100000 loss 0.13898347337916495\n",
      "step 101000 loss 0.148075661749579\n",
      "step 102000 loss 0.1315577950561419\n",
      "step 103000 loss 0.13159158075414598\n",
      "step 104000 loss 0.14378848267253488\n",
      "step 105000 loss 0.130457604277879\n",
      "step 106000 loss 0.14701203173864633\n",
      "step 107000 loss 0.13870403469353915\n",
      "step 108000 loss 0.13982239307742567\n",
      "step 109000 loss 0.12990637144632639\n",
      "step 110000 loss 0.14121706788707525\n",
      "error 110366 CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 4.00 GiB total capacity; 2.34 GiB already allocated; 56.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 111000 loss 0.14166765512432902\n",
      "error 111558 CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 4.00 GiB total capacity; 2.46 GiB already allocated; 78.20 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 112000 loss 0.12250811897590756\n",
      "step 113000 loss 0.13657323824055492\n",
      "step 114000 loss 0.13062375817634164\n",
      "step 115000 loss 0.13479456517193467\n",
      "step 116000 loss 0.12480808864161372\n",
      "step 117000 loss 0.14175010169669985\n",
      "step 118000 loss 0.13443568972079084\n",
      "step 119000 loss 0.12855903386417777\n",
      "step 120000 loss 0.13370124188251795\n",
      "step 121000 loss 0.1258003039546311\n",
      "step 122000 loss 0.13451826660335064\n",
      "step 123000 loss 0.13131440086476504\n",
      "step 124000 loss 0.12867325462959706\n",
      "step 125000 loss 0.1342531214784831\n",
      "step 126000 loss 0.13315761032048612\n",
      "step 127000 loss 0.13098808928579092\n",
      "step 128000 loss 0.14041186919528992\n",
      "error 128439 CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 4.00 GiB total capacity; 2.57 GiB already allocated; 4.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 129000 loss 0.12527855223510415\n",
      "step 130000 loss 0.13614457110781222\n",
      "step 131000 loss 0.12926072251424192\n",
      "step 132000 loss 0.1351702928855084\n",
      "step 133000 loss 0.13719116858486086\n",
      "step 134000 loss 0.12764762246841566\n",
      "step 135000 loss 0.13486053983587773\n",
      "step 136000 loss 0.13758481562044472\n",
      "step 137000 loss 0.12096639063302428\n",
      "step 138000 loss 0.13448526083584875\n",
      "error 138974 CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 4.00 GiB total capacity; 2.48 GiB already allocated; 88.20 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 139000 loss 0.12933293749205768\n",
      "error 139996 CUDA out of memory. Tried to allocate 146.00 MiB (GPU 0; 4.00 GiB total capacity; 1.62 GiB already allocated; 104.20 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 140000 loss 0.12214545883238316\n",
      "error 140475 CUDA out of memory. Tried to allocate 138.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 84.20 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 140611 CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 4.00 GiB total capacity; 2.49 GiB already allocated; 26.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 141000 loss 0.13725962405418976\n",
      "step 142000 loss 0.13063520621415228\n",
      "step 143000 loss 0.1421589503949508\n",
      "step 144000 loss 0.13043211060855536\n",
      "step 145000 loss 0.12827416015416385\n",
      "error 145908 CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 4.00 GiB total capacity; 2.61 GiB already allocated; 12.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 146000 loss 0.12519251561292913\n",
      "step 147000 loss 0.1368513305429369\n",
      "error 147164 CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.61 GiB already allocated; 16.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 148000 loss 0.12670453209988772\n",
      "step 149000 loss 0.13406782924104482\n",
      "step 150000 loss 0.12672919677011668\n",
      "error 150568 CUDA out of memory. Tried to allocate 156.00 MiB (GPU 0; 4.00 GiB total capacity; 2.41 GiB already allocated; 100.20 MiB free; 2.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 151000 loss 0.13553155801258981\n",
      "step 152000 loss 0.13515825170557946\n",
      "error 152674 CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 204.80 KiB free; 2.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 153000 loss 0.1241980385221541\n",
      "step 154000 loss 0.12694220704399048\n",
      "step 155000 loss 0.12535653560236096\n",
      "step 156000 loss 0.13689898052439092\n",
      "error 156437 CUDA out of memory. Tried to allocate 126.00 MiB (GPU 0; 4.00 GiB total capacity; 2.54 GiB already allocated; 24.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 157000 loss 0.1370439844429493\n",
      "error 157060 CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 4.00 GiB total capacity; 2.50 GiB already allocated; 106.20 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 158000 loss 0.13235423298738896\n",
      "error 158459 CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 4.00 GiB total capacity; 2.44 GiB already allocated; 80.20 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 159000 loss 0.13166044672112912\n",
      "error 159732 CUDA out of memory. Tried to allocate 362.00 MiB (GPU 0; 4.00 GiB total capacity; 2.31 GiB already allocated; 176.20 MiB free; 2.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 160000 loss 0.12863297936320306\n",
      "error 160652 CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 4.00 GiB total capacity; 2.59 GiB already allocated; 24.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 161000 loss 0.13063944404851646\n",
      "step 162000 loss 0.13469328630529345\n",
      "step 163000 loss 0.13236894042231143\n",
      "error 163046 CUDA out of memory. Tried to allocate 2.40 GiB (GPU 0; 4.00 GiB total capacity; 210.15 MiB already allocated; 2.37 GiB free; 282.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 164000 loss 0.1405786306122318\n",
      "step 165000 loss 0.13054531861282886\n",
      "error 165194 CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 4.00 GiB total capacity; 2.36 GiB already allocated; 144.20 MiB free; 2.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 166000 loss 0.13780934785492718\n",
      "step 167000 loss 0.13596502013877035\n",
      "error 167568 CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 4.00 GiB total capacity; 2.51 GiB already allocated; 76.20 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 168000 loss 0.12285740908328444\n",
      "step 169000 loss 0.13396336648613213\n",
      "error 169830 CUDA out of memory. Tried to allocate 138.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 86.20 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 170000 loss 0.13173975272011013\n",
      "step 171000 loss 0.13083548041805626\n",
      "step 172000 loss 0.12804919092729689\n",
      "error 172022 CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 4.00 GiB total capacity; 2.46 GiB already allocated; 76.20 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 173000 loss 0.13625038466323167\n",
      "step 174000 loss 0.1345306509686634\n",
      "step 175000 loss 0.12982912039663644\n",
      "step 176000 loss 0.13219514826685191\n",
      "step 177000 loss 0.1244196447795257\n",
      "step 178000 loss 0.12144583887141198\n",
      "step 179000 loss 0.1310137830367312\n",
      "step 180000 loss 0.12841969789937138\n",
      "step 181000 loss 0.13411929146572948\n",
      "step 182000 loss 0.1353993016751483\n",
      "step 183000 loss 0.1365689382618293\n",
      "error 183340 CUDA out of memory. Tried to allocate 304.00 MiB (GPU 0; 4.00 GiB total capacity; 2.37 GiB already allocated; 190.20 MiB free; 2.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 183959 CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 4.00 GiB total capacity; 2.33 GiB already allocated; 226.20 MiB free; 2.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 184000 loss 0.13026669052802026\n",
      "step 185000 loss 0.12825489594601094\n",
      "step 186000 loss 0.1282464880142361\n",
      "step 187000 loss 0.12483413502108305\n",
      "step 188000 loss 0.13471558112930507\n",
      "step 189000 loss 0.13610349807143213\n",
      "error 189467 CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 4.00 GiB total capacity; 2.55 GiB already allocated; 38.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 190000 loss 0.12457984532695264\n",
      "step 191000 loss 0.12957004991639406\n",
      "error 191691 CUDA out of memory. Tried to allocate 116.00 MiB (GPU 0; 4.00 GiB total capacity; 2.49 GiB already allocated; 90.20 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 192000 loss 0.1255668413117528\n",
      "step 193000 loss 0.12870807736832648\n",
      "step 194000 loss 0.13316477357968687\n",
      "step 195000 loss 0.13516202640067787\n",
      "step 196000 loss 0.1352424515346065\n",
      "step 197000 loss 0.12013175839185715\n",
      "step 198000 loss 0.1241946041835472\n",
      "step 199000 loss 0.12619599061179906\n",
      "step 200000 loss 0.13392212587874383\n",
      "step 201000 loss 0.12687422308698296\n",
      "step 202000 loss 0.1396926735136658\n",
      "step 203000 loss 0.12611808873992414\n",
      "step 204000 loss 0.13116981998831034\n",
      "step 205000 loss 0.13170791402831675\n",
      "step 206000 loss 0.13823698851652444\n",
      "step 207000 loss 0.12428398659732193\n",
      "step 208000 loss 0.13639217920042573\n",
      "step 209000 loss 0.1247930075544864\n",
      "step 210000 loss 0.12296367700770497\n",
      "step 211000 loss 0.13141709265392273\n",
      "step 212000 loss 0.13067718520667404\n",
      "step 213000 loss 0.12688524150941521\n",
      "error 213191 CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 46.20 MiB free; 2.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 214000 loss 0.12675094590056687\n",
      "step 215000 loss 0.12797484174277635\n",
      "error 215667 CUDA out of memory. Tried to allocate 230.00 MiB (GPU 0; 4.00 GiB total capacity; 2.44 GiB already allocated; 138.20 MiB free; 2.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 216000 loss 0.12491484171524644\n",
      "step 217000 loss 0.1276680974289775\n",
      "step 218000 loss 0.13018016526475548\n",
      "step 219000 loss 0.1285505546424538\n",
      "step 220000 loss 0.12921465805452317\n",
      "step 221000 loss 0.12795297065377234\n",
      "step 222000 loss 0.13329555353987962\n",
      "error 222737 CUDA out of memory. Tried to allocate 180.00 MiB (GPU 0; 4.00 GiB total capacity; 2.44 GiB already allocated; 18.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 223000 loss 0.1312027073018253\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'00'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23716/2342345743.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mxx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_sents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0myy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnoiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medit_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mp_keep\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23716/2342345743.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mxx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_sents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0myy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnoiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medit_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mp_keep\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23716/1095905948.py\u001b[0m in \u001b[0;36madd_noise\u001b[1;34m(self, text, min_edits, max_edits, p_exit, p_insert, temp, p_randchar, edit_rate, eps)\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 replacement = random.choices(\n\u001b[1;32m---> 76\u001b[1;33m                     \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew2olds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m                     \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0meps\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew2olds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                 )[0]\n",
      "\u001b[1;31mKeyError\u001b[0m: '00'"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for i in trange(1_000_000):\n",
    "    if random.random() < share_real:\n",
    "        batch = df_train.sample(bs)\n",
    "        xx, yy = batch.trash2.tolist(), batch.clean2.tolist()\n",
    "    else:\n",
    "        xx = random.sample(clean_sents, bs)\n",
    "        yy = [noiser.add_noise(text, edit_rate=0.05) if random.random() < p_keep else text for text in xx]\n",
    "    \n",
    "    try:\n",
    "        x = tokenizer(xx, padding=True, return_tensors='pt').to(model.device)\n",
    "        y = tokenizer(yy, padding=True, return_tensors='pt').to(model.device)\n",
    "\n",
    "        y.input_ids[y.input_ids == 0] = -100\n",
    "        loss = model(\n",
    "            input_ids=x.input_ids,\n",
    "            attention_mask=x.attention_mask,\n",
    "            labels=y.input_ids,\n",
    "            decoder_attention_mask=y.attention_mask,\n",
    "            return_dict=True\n",
    "        ).loss\n",
    "        loss.backward()\n",
    "        if i % grad_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "    except RuntimeError as e:\n",
    "        loss = None\n",
    "        optimizer.zero_grad()\n",
    "        cleanup()\n",
    "        print('error', i, e)\n",
    "        \n",
    "    if i % report_steps == 0:\n",
    "        print('step', i, 'loss', np.mean(losses[-report_steps:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "27df9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "ff7e924e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8fbf289a95e4403baee8211e70b07ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_small['fixed2'] = [fix(text, num_beams=2) for text in tqdm(dev_small.trash2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8e0a36a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distance               1.2900\n",
       "normalized_distance    0.0175\n",
       "edit_max_cldiff        0.4600\n",
       "edit_max_lendiff       0.0300\n",
       "change_amount          0.5900\n",
       "new_diff               1.3800\n",
       "dtype: float64"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_small['change_amount'] = dev_small.apply(lambda row: textdistance.levenshtein.distance(row.trash2, row.fixed2), axis=1)\n",
    "dev_small['new_diff'] = dev_small.apply(lambda row: textdistance.levenshtein.distance(row.clean2, row.fixed2), axis=1)\n",
    "\n",
    "dev_small.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "af50f3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "0.17829457364341084\n"
     ]
    }
   ],
   "source": [
    "cnd = dev_small.new_diff * (dev_small.change_amount < 5) + dev_small.distance * (dev_small.change_amount >= 5)\n",
    "print(cnd.sum())\n",
    "print(1 - cnd.sum() / dev_small.distance.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4990c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
