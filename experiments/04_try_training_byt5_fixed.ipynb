{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb558e3",
   "metadata": {},
   "source": [
    "Inspired by https://github.com/lingjzhu/CharsiuG2P. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73ae3c",
   "metadata": {},
   "source": [
    "* Baseline (just a small byt5): 0.387 reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32141927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3345d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('charsiu/g2p_multilingual_byT5_tiny_8_layers_100')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/byt5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee9a066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ˈtʃɑɹ', 'ˈsiu', 'ˈɪs', 'ˈɑ', 'ˈkæntoʊˌniz', 'ˈstaɪɫ', 'ˈɔf', 'ˈbɑɹbɪkjud', 'ˈpɔɹk']\n"
     ]
    }
   ],
   "source": [
    "# tokenized English words\n",
    "words = ['Char', 'siu', 'is', 'a', 'Cantonese', 'style', 'of', 'barbecued', 'pork']\n",
    "words = ['<eng-us>: '+i for i in words]\n",
    "\n",
    "out = tokenizer(words,padding=True,add_special_tokens=False,return_tensors='pt')\n",
    "\n",
    "preds = model.generate(**out,num_beams=1) # We do not find beam search helpful. Greedy decoding is enough. \n",
    "phones = tokenizer.batch_decode(preds.tolist(),skip_special_tokens=True)\n",
    "print(phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90fbdc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ɡəspɐdak', 'ja', 'nʲe', 'jeɫ', 'ʂɛsʲtʲ', 'dʲnʲej', 't͡sɛɫɨx', 'tsʰɪŋ˧˥', 'dʲnʲej']\n",
      "Wall time: 237 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "words = 'Господа, я не ел шесть дней целых 6 дней'.split()\n",
    "words = ['<rus>: '+i for i in words]\n",
    "\n",
    "out = tokenizer(words, padding=True, add_special_tokens=False, return_tensors='pt')\n",
    "\n",
    "preds = model.generate(**out,num_beams=1) # We do not find beam search helpful. Greedy decoding is enough. \n",
    "phones = tokenizer.batch_decode(preds.tolist(),skip_special_tokens=True)\n",
    "print(phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f160efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ð', '¿', 'Ñ', '\\x80', 'Ð', '¸', 'Ð', '²', 'Ð', 'µ', 'Ñ', '\\x82', '</s>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer('привет').input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd21ae",
   "metadata": {},
   "source": [
    "# Loading the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ee7362",
   "metadata": {},
   "source": [
    "## The original sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75462b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aec2533",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trash</th>\n",
       "      <th>clean</th>\n",
       "      <th>trash2</th>\n",
       "      <th>clean2</th>\n",
       "      <th>distance</th>\n",
       "      <th>normalized_distance</th>\n",
       "      <th>split</th>\n",
       "      <th>edit_max_cldiff</th>\n",
       "      <th>edit_max_lendiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Шунда ук әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...</td>\n",
       "      <td>Шунда уҡ әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...</td>\n",
       "      <td>Шунда ук әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...</td>\n",
       "      <td>Шунда уҡ әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Унан беҙ өсөбөҙ ҙә ултырғыстарға ултырабыҙ.</td>\n",
       "      <td>Унан беҙ әсәбеҙ ҙә ултырғыстарға ултырабыҙ.</td>\n",
       "      <td>Унан беҙ өсөбөҙ ҙә ултырғыстарға ултырабыҙ.</td>\n",
       "      <td>Унан беҙ әсәбеҙ ҙә ултырғыстарға ултырабыҙ.</td>\n",
       "      <td>3</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>«Иҫән-Һау ғына тороғоҙ инде», - тип бышылдай у...</td>\n",
       "      <td>«Иҫән-һау ғына тороғоҙ инде», - тип бышылдай у...</td>\n",
       "      <td>«Иҫән-Һау ғына тороғоҙ инде», - тип бышылдай у...</td>\n",
       "      <td>«Иҫән-һау ғына тороғоҙ инде», - тип бышылдай у...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>dev</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Минең генә бер кешем дә юҡ, тип шунда уҡ танау...</td>\n",
       "      <td>Минең генә бер кешем дә юҡ, - тип шунда уҡ тан...</td>\n",
       "      <td>Минең генә бер кешем дә юҡ, тип шунда уҡ танау...</td>\n",
       "      <td>Минең генә бер кешем дә юҡ, - тип шунда уҡ тан...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...</td>\n",
       "      <td>Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...</td>\n",
       "      <td>Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...</td>\n",
       "      <td>Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23886</th>\n",
       "      <td>Эҫтәрендә бүре үк оломаһа ла, эттәр шыңшый баш...</td>\n",
       "      <td>Эстәрендә бүре үк оломаһа ла, эттәр шыңшый баш...</td>\n",
       "      <td>Эҫтәрендә бүре үк оломаһа ла, эттәр шыңшый баш...</td>\n",
       "      <td>Эстәрендә бүре үк оломаһа ла, эттәр шыңшый баш...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>dev</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23887</th>\n",
       "      <td>Үткән йәйҙә яман томра көндө Кәҙерғол төбәгенд...</td>\n",
       "      <td>Үткән йәйҙә яман томра көндө Ҡәҙерғол төбәгенд...</td>\n",
       "      <td>Үткән йәйҙә яман томра көндө Кәҙерғол төбәгенд...</td>\n",
       "      <td>Үткән йәйҙә яман томра көндө Ҡәҙерғол төбәгенд...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23888</th>\n",
       "      <td>Кайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...</td>\n",
       "      <td>Ҡайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...</td>\n",
       "      <td>Кайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...</td>\n",
       "      <td>Ҡайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23889</th>\n",
       "      <td>Кыш урталарында бер көн Әбдрәшит ат аҙбарынан ...</td>\n",
       "      <td>Ҡыш урталарында бер көн Әбдрәшит ат аҙбарынан ...</td>\n",
       "      <td>Кыш урталарында бер көн Әбдрәшит ат аҙбарынан ...</td>\n",
       "      <td>Ҡыш урталарында бер көн Әбдрәшит ат аҙбарынан ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23890</th>\n",
       "      <td>Шатлыҡ тигәнебеҙ юғалған салбар ҡупшы ғына ите...</td>\n",
       "      <td>Шатлыҡ тигәнебеҙ юғалған салбар - ҡупшы ғына и...</td>\n",
       "      <td>Шатлыҡ тигәнебеҙ юғалған салбар ҡупшы ғына ите...</td>\n",
       "      <td>Шатлыҡ тигәнебеҙ юғалған салбар - ҡупшы ғына и...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23891 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   trash  \\\n",
       "0      Шунда ук әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...   \n",
       "1            Унан беҙ өсөбөҙ ҙә ултырғыстарға ултырабыҙ.   \n",
       "2      «Иҫән-Һау ғына тороғоҙ инде», - тип бышылдай у...   \n",
       "3      Минең генә бер кешем дә юҡ, тип шунда уҡ танау...   \n",
       "4      Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...   \n",
       "...                                                  ...   \n",
       "23886  Эҫтәрендә бүре үк оломаһа ла, эттәр шыңшый баш...   \n",
       "23887  Үткән йәйҙә яман томра көндө Кәҙерғол төбәгенд...   \n",
       "23888  Кайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...   \n",
       "23889  Кыш урталарында бер көн Әбдрәшит ат аҙбарынан ...   \n",
       "23890  Шатлыҡ тигәнебеҙ юғалған салбар ҡупшы ғына ите...   \n",
       "\n",
       "                                                   clean  \\\n",
       "0      Шунда уҡ әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...   \n",
       "1            Унан беҙ әсәбеҙ ҙә ултырғыстарға ултырабыҙ.   \n",
       "2      «Иҫән-һау ғына тороғоҙ инде», - тип бышылдай у...   \n",
       "3      Минең генә бер кешем дә юҡ, - тип шунда уҡ тан...   \n",
       "4      Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...   \n",
       "...                                                  ...   \n",
       "23886  Эстәрендә бүре үк оломаһа ла, эттәр шыңшый баш...   \n",
       "23887  Үткән йәйҙә яман томра көндө Ҡәҙерғол төбәгенд...   \n",
       "23888  Ҡайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...   \n",
       "23889  Ҡыш урталарында бер көн Әбдрәшит ат аҙбарынан ...   \n",
       "23890  Шатлыҡ тигәнебеҙ юғалған салбар - ҡупшы ғына и...   \n",
       "\n",
       "                                                  trash2  \\\n",
       "0      Шунда ук әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...   \n",
       "1            Унан беҙ өсөбөҙ ҙә ултырғыстарға ултырабыҙ.   \n",
       "2      «Иҫән-Һау ғына тороғоҙ инде», - тип бышылдай у...   \n",
       "3      Минең генә бер кешем дә юҡ, тип шунда уҡ танау...   \n",
       "4      Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...   \n",
       "...                                                  ...   \n",
       "23886  Эҫтәрендә бүре үк оломаһа ла, эттәр шыңшый баш...   \n",
       "23887  Үткән йәйҙә яман томра көндө Кәҙерғол төбәгенд...   \n",
       "23888  Кайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...   \n",
       "23889  Кыш урталарында бер көн Әбдрәшит ат аҙбарынан ...   \n",
       "23890  Шатлыҡ тигәнебеҙ юғалған салбар ҡупшы ғына ите...   \n",
       "\n",
       "                                                  clean2  distance  \\\n",
       "0      Шунда уҡ әсәйемдең тоҡсайын, төйөнсөктәрен күҙ...         1   \n",
       "1            Унан беҙ әсәбеҙ ҙә ултырғыстарға ултырабыҙ.         3   \n",
       "2      «Иҫән-һау ғына тороғоҙ инде», - тип бышылдай у...         1   \n",
       "3      Минең генә бер кешем дә юҡ, - тип шунда уҡ тан...         2   \n",
       "4      Ай йөрөгән, ти, йыл йөрөгән, ти, батыр, ете та...         1   \n",
       "...                                                  ...       ...   \n",
       "23886  Эстәрендә бүре үк оломаһа ла, эттәр шыңшый баш...         1   \n",
       "23887  Үткән йәйҙә яман томра көндө Ҡәҙерғол төбәгенд...         1   \n",
       "23888  Ҡайтыр алдынан салбарҙы эҙләй башлаһа, таба ал...         1   \n",
       "23889  Ҡыш урталарында бер көн Әбдрәшит ат аҙбарынан ...         1   \n",
       "23890  Шатлыҡ тигәнебеҙ юғалған салбар - ҡупшы ғына и...         2   \n",
       "\n",
       "       normalized_distance  split  edit_max_cldiff  edit_max_lendiff  \n",
       "0                 0.015385  train                1                 0  \n",
       "1                 0.069767   test                1                 0  \n",
       "2                 0.014085    dev                1                 0  \n",
       "3                 0.029412  train                0                 0  \n",
       "4                 0.012500  train                1                 0  \n",
       "...                    ...    ...              ...               ...  \n",
       "23886             0.020000    dev                1                 0  \n",
       "23887             0.009524  train                1                 0  \n",
       "23888             0.020000  train                1                 0  \n",
       "23889             0.009174  train                1                 0  \n",
       "23890             0.021053  train                0                 0  \n",
       "\n",
       "[23891 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig = pd.read_csv('../data/spellchecker_dataset_split.tsv', sep='\\t')\n",
    "df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecfb77d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14382, 9)\n",
      "(14171, 9)\n",
      "(14085, 9)\n"
     ]
    }
   ],
   "source": [
    "df_orig_train = df_orig[(df_orig.split=='train')]\n",
    "print(df_orig_train.shape)\n",
    "\n",
    "df_orig_train = df_orig_train[df_orig_train.edit_max_cldiff <= 3]\n",
    "print(df_orig_train.shape)\n",
    "df_orig_train = df_orig_train[df_orig_train.edit_max_lendiff <= 1].copy()\n",
    "print(df_orig_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48011163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4611, 9)\n"
     ]
    }
   ],
   "source": [
    "df_orig_dev = df_orig[(df_orig.split=='dev') & (df_orig.edit_max_cldiff <= 3) & (df_orig.edit_max_lendiff <= 1)]\n",
    "print(df_orig_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b78c2c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_lens = pd.Series([len(s) for s in tokenizer(df_orig_train.trash2.tolist())['input_ids']])\n",
    "new_lens = pd.Series([len(s) for s in tokenizer(df_orig_train.clean2.tolist())['input_ids']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362970c4",
   "metadata": {},
   "source": [
    "## Artificial replacements (todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c664b",
   "metadata": {},
   "source": [
    "Clone https://github.com/nevmenandr/bashkir-corpus/ nearby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df01cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4113a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/clean_bk_sents.txt', 'r') as f:\n",
    "    clean_sents = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8f59f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1605495\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef827a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd9f5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21226e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_orig_train.copy().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb5ed680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'trash', 'clean', 'trash2', 'clean2', 'distance',\n",
       "       'normalized_distance', 'split', 'edit_max_cldiff', 'edit_max_lendiff'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0486b07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c11b296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1cc0302",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5a4486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "report_steps = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abc7a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88e8dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix(text, num_beams=1, max_length='auto', min_length='auto', **kwargs):\n",
    "    out = tokenizer(text, padding=True, return_tensors='pt').to(model.device)\n",
    "    n = out.input_ids.shape[1]\n",
    "    if max_length == 'auto':\n",
    "        max_length = int(n * 1.02 + 4)\n",
    "    if min_length == 'auto':\n",
    "        min_length = max(1, int(n * 0.98 - 4))\n",
    "    preds = model.generate(**out,num_beams=num_beams, max_length=max_length, min_length=min_length, **kwargs)\n",
    "    result = tokenizer.decode(preds[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98c3d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_small = df_orig_dev.sample(100, random_state=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6ae21",
   "metadata": {},
   "source": [
    "# Now try adding synthetic noise to the clean training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "abc376d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'noisers' from 'C:\\\\Users\\\\david\\\\YandexDisk\\\\code\\\\NLP\\\\bashkort-spellchecker\\\\experiments\\\\noisers.py'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import noisers\n",
    "from importlib import reload\n",
    "reload(noisers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f9889ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from noisers import Noiser, add_simple_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0ebf748",
   "metadata": {},
   "outputs": [],
   "source": [
    "noiser = Noiser.load('noise_model_v1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9100a870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7322a0ff6948fbb358f642f2e68827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1605495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "chars_cnt = Counter(c for sent in tqdm(clean_sents) for c in sent)\n",
    "all_chars = list(chars_cnt.keys())\n",
    "len(chars_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49a912be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = random.choice(clean_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aac40965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Әҙип һүҙе милләттәрҙең йөрәген аса!\n",
      "Әҙип һүҙе милләттәрҙвң йөрәген аса!\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print(noiser.add_noise(text, edit_rate=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6dc8eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d745f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "share_real = 0.1\n",
    "share_noiser = 0.5\n",
    "p_keep = 0.5\n",
    "grad_steps = 8\n",
    "report_steps = 1000\n",
    "bs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54d653e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f909c2e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5c0c23a347493c85705242939d1b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 1.5799298595190048\n",
      "step 1000 loss 1.5785965538024902\n",
      "step 2000 loss 1.5743405665159225\n",
      "error 2143 CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 4.00 GiB total capacity; 2.26 GiB already allocated; 168.20 MiB free; 2.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 2639 CUDA out of memory. Tried to allocate 2.40 GiB (GPU 0; 4.00 GiB total capacity; 210.20 MiB already allocated; 2.26 GiB free; 396.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 3000 loss 1.566542668223381\n",
      "step 4000 loss 1.5563196305036544\n",
      "step 5000 loss 1.545657940864563\n",
      "step 6000 loss 1.5398989090919495\n",
      "step 7000 loss 1.539822576880455\n",
      "step 8000 loss 1.5328044068217277\n",
      "step 9000 loss 1.5179099105596543\n",
      "step 10000 loss 1.526239313840866\n",
      "step 11000 loss 1.5163540083765983\n",
      "step 12000 loss 1.5054893945455552\n",
      "step 13000 loss 1.4971007786989212\n",
      "step 14000 loss 1.4953845739364624\n",
      "error 14106 CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 4.00 GiB total capacity; 2.54 GiB already allocated; 72.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 15000 loss 1.489492159128189\n",
      "step 16000 loss 1.488424580514431\n",
      "step 17000 loss 1.489682767033577\n",
      "step 18000 loss 1.4703380815386773\n",
      "step 19000 loss 1.4657676117420197\n",
      "step 20000 loss 1.4673502333164214\n",
      "step 21000 loss 1.4680037635564804\n",
      "error 21373 CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 4.00 GiB total capacity; 2.46 GiB already allocated; 80.20 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 22000 loss 1.4551027095913887\n",
      "step 23000 loss 1.4369300780296326\n",
      "step 24000 loss 1.4271113530993462\n",
      "step 25000 loss 1.3939452007412911\n",
      "step 26000 loss 1.397028764307499\n",
      "step 27000 loss 1.4065900152921678\n",
      "error 27477 CUDA out of memory. Tried to allocate 466.00 MiB (GPU 0; 4.00 GiB total capacity; 1.90 GiB already allocated; 360.20 MiB free; 2.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 28000 loss 1.3779610488414764\n",
      "step 29000 loss 1.3605613909959793\n",
      "step 30000 loss 1.3573540562391282\n",
      "step 31000 loss 1.350209857404232\n",
      "step 32000 loss 1.3454683784246444\n",
      "step 33000 loss 1.317317902803421\n",
      "step 34000 loss 1.3036375313699244\n",
      "step 35000 loss 1.285353754043579\n",
      "step 36000 loss 1.2625150952935218\n",
      "step 37000 loss 1.2539473347663879\n",
      "error 37399 CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 4.00 GiB total capacity; 2.24 GiB already allocated; 360.20 MiB free; 2.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 38000 loss 1.2259564493298531\n",
      "step 39000 loss 1.1820635872483254\n",
      "error 39606 CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 4.00 GiB total capacity; 2.46 GiB already allocated; 54.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 40000 loss 1.2047521996200086\n",
      "step 41000 loss 1.1527466540038587\n",
      "error 41704 CUDA out of memory. Tried to allocate 200.00 MiB (GPU 0; 4.00 GiB total capacity; 2.47 GiB already allocated; 8.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 42000 loss 1.1511327055692673\n",
      "step 43000 loss 1.11919412830472\n",
      "step 44000 loss 1.098051234126091\n",
      "step 45000 loss 1.0661956101506949\n",
      "step 46000 loss 1.0644871353507042\n",
      "step 47000 loss 1.0462651802003384\n",
      "step 48000 loss 1.016543443530798\n",
      "step 49000 loss 0.9953150996416807\n",
      "step 50000 loss 0.9674924291819335\n",
      "step 51000 loss 0.9581204105317592\n",
      "error 51964 CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 4.00 GiB total capacity; 2.53 GiB already allocated; 32.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 52000 loss 0.9347727634906768\n",
      "error 52646 CUDA out of memory. Tried to allocate 204.00 MiB (GPU 0; 4.00 GiB total capacity; 2.43 GiB already allocated; 112.20 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 53000 loss 0.9098249440193176\n",
      "step 54000 loss 0.8961902933716774\n",
      "step 55000 loss 0.8865260403007269\n",
      "error 55163 CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 4.00 GiB total capacity; 2.41 GiB already allocated; 176.20 MiB free; 2.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 56000 loss 0.8561228320300579\n",
      "step 57000 loss 0.8458295051082969\n",
      "step 58000 loss 0.8197050805985927\n",
      "step 59000 loss 0.8013094810023904\n",
      "step 60000 loss 0.786808021709323\n",
      "step 61000 loss 0.7741182953193784\n",
      "step 62000 loss 0.7687822370529175\n",
      "step 63000 loss 0.7707721270918846\n",
      "step 64000 loss 0.7422472723312676\n",
      "step 65000 loss 0.7428600001037121\n",
      "step 66000 loss 0.7169622986093164\n",
      "step 67000 loss 0.7402435247749091\n",
      "error 67319 CUDA out of memory. Tried to allocate 318.00 MiB (GPU 0; 4.00 GiB total capacity; 2.05 GiB already allocated; 172.20 MiB free; 2.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 68000 loss 0.6967764520421624\n",
      "step 69000 loss 0.6860176173299551\n",
      "step 70000 loss 0.6950042600668966\n",
      "error 70881 CUDA out of memory. Tried to allocate 106.00 MiB (GPU 0; 4.00 GiB total capacity; 2.44 GiB already allocated; 58.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 71000 loss 0.706934761699289\n",
      "error 71302 CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 4.00 GiB total capacity; 2.57 GiB already allocated; 16.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 71554 CUDA out of memory. Tried to allocate 354.00 MiB (GPU 0; 4.00 GiB total capacity; 2.27 GiB already allocated; 282.20 MiB free; 2.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 72000 loss 0.6616572772301733\n",
      "step 73000 loss 0.6726327827163041\n",
      "step 74000 loss 0.6496837920323014\n",
      "step 75000 loss 0.6390513949841261\n",
      "step 76000 loss 0.6144335942333564\n",
      "step 77000 loss 0.6422520516999066\n",
      "step 78000 loss 0.6003707310780882\n",
      "step 79000 loss 0.6159212396405638\n",
      "step 80000 loss 0.6044390835650265\n",
      "step 81000 loss 0.5935543990135193\n",
      "step 82000 loss 0.5823246492221952\n",
      "step 83000 loss 0.57269479675591\n",
      "error 83362 CUDA out of memory. Tried to allocate 206.00 MiB (GPU 0; 4.00 GiB total capacity; 1.68 GiB already allocated; 192.20 MiB free; 2.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 84000 loss 0.5776040833257139\n",
      "step 85000 loss 0.5747117669843137\n",
      "step 86000 loss 0.556412426430732\n",
      "step 87000 loss 0.560354130776599\n",
      "step 88000 loss 0.5596464613322168\n",
      "step 89000 loss 0.5371074318196625\n",
      "error 89133 CUDA out of memory. Tried to allocate 700.00 MiB (GPU 0; 4.00 GiB total capacity; 2.24 GiB already allocated; 314.20 MiB free; 2.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 90000 loss 0.5644116847887635\n",
      "step 91000 loss 0.518633094843477\n",
      "error 91710 CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 4.00 GiB total capacity; 2.34 GiB already allocated; 132.20 MiB free; 2.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 92000 loss 0.5183962627705186\n",
      "error 92500 CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 4.00 GiB total capacity; 2.40 GiB already allocated; 14.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 93000 loss 0.5181792296878993\n",
      "error 93805 CUDA out of memory. Tried to allocate 646.00 MiB (GPU 0; 4.00 GiB total capacity; 2.08 GiB already allocated; 528.20 MiB free; 2.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 94000 loss 0.5123265453986824\n",
      "step 95000 loss 0.5249187503606081\n",
      "step 96000 loss 0.5437750805933028\n",
      "step 97000 loss 0.5034499587360769\n",
      "error 97307 CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 4.00 GiB total capacity; 2.38 GiB already allocated; 20.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 98000 loss 0.49737613045983015\n",
      "step 99000 loss 0.47765577711258084\n",
      "step 100000 loss 0.4841717477384955\n",
      "step 101000 loss 0.4921885988302529\n",
      "step 102000 loss 0.4730588621404022\n",
      "error 102919 CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 4.00 GiB total capacity; 2.48 GiB already allocated; 72.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 103000 loss 0.47064722854457797\n",
      "step 104000 loss 0.46639451865293086\n",
      "error 104140 CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 4.00 GiB total capacity; 2.58 GiB already allocated; 38.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 105000 loss 0.45516030734218654\n",
      "step 106000 loss 0.46113941422104837\n",
      "step 107000 loss 0.4558859235793352\n",
      "error 107510 CUDA out of memory. Tried to allocate 200.00 MiB (GPU 0; 4.00 GiB total capacity; 2.47 GiB already allocated; 14.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 108000 loss 0.4604084270540625\n",
      "step 109000 loss 0.45751230202428994\n",
      "step 110000 loss 0.443225753727369\n",
      "error 110716 CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 4.00 GiB total capacity; 2.44 GiB already allocated; 182.20 MiB free; 2.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 111000 loss 0.42714628427103163\n",
      "step 112000 loss 0.4286899897418916\n",
      "step 113000 loss 0.42291164573840795\n",
      "step 114000 loss 0.42716208874061706\n",
      "step 115000 loss 0.4280529771130532\n",
      "error 115770 CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 4.00 GiB total capacity; 2.58 GiB already allocated; 12.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 116000 loss 0.41091345307044685\n",
      "step 117000 loss 0.42146277316566555\n",
      "error 117950 CUDA out of memory. Tried to allocate 480.00 MiB (GPU 0; 4.00 GiB total capacity; 1.96 GiB already allocated; 304.20 MiB free; 2.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 118000 loss 0.42525213250517846\n",
      "step 119000 loss 0.4202181264422834\n",
      "step 120000 loss 0.3994706459492445\n",
      "step 121000 loss 0.39715296612679957\n",
      "step 122000 loss 0.40558553631044925\n",
      "step 123000 loss 0.3913363046236336\n",
      "step 124000 loss 0.40536144153960046\n",
      "step 125000 loss 0.39737916123867034\n",
      "error 125071 CUDA out of memory. Tried to allocate 106.00 MiB (GPU 0; 4.00 GiB total capacity; 2.42 GiB already allocated; 80.20 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 126000 loss 0.39613230114243925\n",
      "step 127000 loss 0.39198714833892884\n",
      "step 128000 loss 0.3936644054437056\n",
      "error 128908 CUDA out of memory. Tried to allocate 88.00 MiB (GPU 0; 4.00 GiB total capacity; 2.51 GiB already allocated; 2.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 129000 loss 0.3797852885639295\n",
      "step 130000 loss 0.3928067941982299\n",
      "step 131000 loss 0.3825104474527761\n",
      "step 132000 loss 0.35887476829160003\n",
      "step 133000 loss 0.38644688010960815\n",
      "step 134000 loss 0.36717504264600576\n",
      "step 135000 loss 0.37077506975177676\n",
      "step 136000 loss 0.36310654651466756\n",
      "step 137000 loss 0.3544709511967376\n",
      "step 138000 loss 0.3659482950521633\n",
      "step 139000 loss 0.3548065581470728\n",
      "step 140000 loss 0.3637494892263785\n",
      "step 141000 loss 0.3600988175319508\n",
      "error 141750 CUDA out of memory. Tried to allocate 180.00 MiB (GPU 0; 4.00 GiB total capacity; 2.42 GiB already allocated; 16.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 141845 CUDA out of memory. Tried to allocate 102.00 MiB (GPU 0; 4.00 GiB total capacity; 2.44 GiB already allocated; 88.20 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 142000 loss 0.353659000614658\n",
      "step 143000 loss 0.35250637564715\n",
      "error 143384 CUDA out of memory. Tried to allocate 372.00 MiB (GPU 0; 4.00 GiB total capacity; 2.38 GiB already allocated; 200.20 MiB free; 2.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 143873 CUDA out of memory. Tried to allocate 560.00 MiB (GPU 0; 4.00 GiB total capacity; 2.11 GiB already allocated; 522.20 MiB free; 2.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 144000 loss 0.35484432245232167\n",
      "step 145000 loss 0.34215618593152614\n",
      "step 146000 loss 0.341125864229165\n",
      "step 147000 loss 0.33731805391423403\n",
      "step 148000 loss 0.34590606871340424\n",
      "step 149000 loss 0.33279266507551075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 150000 loss 0.3205536270784214\n",
      "step 151000 loss 0.34117992675397546\n",
      "step 152000 loss 0.3316469591166824\n",
      "step 153000 loss 0.32158420656900855\n",
      "step 154000 loss 0.32227523491624743\n",
      "step 155000 loss 0.3222550076730549\n",
      "step 156000 loss 0.33178687118180095\n",
      "error 156383 CUDA out of memory. Tried to allocate 460.00 MiB (GPU 0; 4.00 GiB total capacity; 1.76 GiB already allocated; 390.20 MiB free; 2.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 157000 loss 0.30821347533911464\n",
      "step 158000 loss 0.31469858943996953\n",
      "step 159000 loss 0.3110806119404733\n",
      "step 160000 loss 0.3067249200092629\n",
      "error 160447 CUDA out of memory. Tried to allocate 420.00 MiB (GPU 0; 4.00 GiB total capacity; 2.22 GiB already allocated; 392.20 MiB free; 2.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 161000 loss 0.30348620171286167\n",
      "step 162000 loss 0.33078507910110055\n",
      "step 163000 loss 0.3139775231126696\n",
      "step 164000 loss 0.3033768400494009\n",
      "step 165000 loss 0.30386163512058556\n",
      "step 166000 loss 0.3046071671200916\n",
      "error 166901 CUDA out of memory. Tried to allocate 88.00 MiB (GPU 0; 4.00 GiB total capacity; 2.51 GiB already allocated; 6.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 167000 loss 0.3057604283969849\n",
      "step 168000 loss 0.30640145783498884\n",
      "step 169000 loss 0.30222816980630157\n",
      "step 170000 loss 0.3101041175434366\n",
      "step 171000 loss 0.3021067229313776\n",
      "step 172000 loss 0.29449960553552956\n",
      "error 172429 CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 4.00 GiB total capacity; 2.34 GiB already allocated; 166.20 MiB free; 2.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 173000 loss 0.2891500653051771\n",
      "step 174000 loss 0.275099081360735\n",
      "step 175000 loss 0.2983636451419443\n",
      "step 176000 loss 0.2852311583859846\n",
      "step 177000 loss 0.2978459735037759\n",
      "error 177586 CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 4.00 GiB total capacity; 2.37 GiB already allocated; 260.20 MiB free; 2.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 178000 loss 0.29299732733238487\n",
      "step 179000 loss 0.27070477783307434\n",
      "step 180000 loss 0.28852820974867793\n",
      "step 181000 loss 0.2774965324392542\n",
      "error 181217 CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 4.00 GiB total capacity; 2.56 GiB already allocated; 56.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 181980 CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 4.00 GiB total capacity; 2.49 GiB already allocated; 66.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 182000 loss 0.2786628902684897\n",
      "step 183000 loss 0.2702202573064715\n",
      "step 184000 loss 0.2680776074444875\n",
      "error 184529 CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.61 GiB already allocated; 6.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 185000 loss 0.26742915919609367\n",
      "step 186000 loss 0.27748008178640154\n",
      "step 187000 loss 0.27825832484290003\n",
      "step 188000 loss 0.26605897648632526\n",
      "step 189000 loss 0.2604481120249256\n",
      "step 190000 loss 0.2524882207904011\n",
      "error 190090 CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 4.00 GiB total capacity; 2.53 GiB already allocated; 70.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 191000 loss 0.26716325983311984\n",
      "step 192000 loss 0.2672418676689267\n",
      "step 193000 loss 0.2627748055271804\n",
      "step 194000 loss 0.26040427924133835\n",
      "step 195000 loss 0.2841359393727034\n",
      "step 196000 loss 0.27404014868848026\n",
      "step 197000 loss 0.26651283127535136\n",
      "step 198000 loss 0.2640046080509201\n",
      "step 199000 loss 0.2637889785161242\n",
      "step 200000 loss 0.2709911521719769\n",
      "step 201000 loss 0.2575866200160235\n",
      "step 202000 loss 0.24657395966444165\n",
      "step 203000 loss 0.2566070282314904\n",
      "step 204000 loss 0.25907789734937253\n",
      "step 205000 loss 0.2425540093285963\n",
      "step 206000 loss 0.2536155779254623\n",
      "step 207000 loss 0.24087050351686776\n",
      "error 207297 CUDA out of memory. Tried to allocate 574.00 MiB (GPU 0; 4.00 GiB total capacity; 2.17 GiB already allocated; 478.20 MiB free; 2.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 208000 loss 0.2556846941960976\n",
      "step 209000 loss 0.25822158987028526\n",
      "step 210000 loss 0.26730323443515225\n",
      "step 211000 loss 0.25217961495602503\n",
      "step 212000 loss 0.2395642826613039\n",
      "error 212675 CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 2.56 GiB already allocated; 40.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 213000 loss 0.24767573870997875\n",
      "error 213617 CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 4.00 GiB total capacity; 2.54 GiB already allocated; 36.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 214000 loss 0.2461765931593254\n",
      "step 215000 loss 0.24931574262026698\n",
      "step 216000 loss 0.24067671406408772\n",
      "step 217000 loss 0.22905564111750573\n",
      "error 217057 CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 4.00 GiB total capacity; 2.56 GiB already allocated; 14.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 218000 loss 0.24699041456077248\n",
      "step 219000 loss 0.2418447194751352\n",
      "step 220000 loss 0.2415762581154704\n",
      "step 221000 loss 0.2384787462502718\n",
      "step 222000 loss 0.23000875134579837\n",
      "error 222764 CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 4.00 GiB total capacity; 2.51 GiB already allocated; 84.20 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 223000 loss 0.22243660338083282\n",
      "step 224000 loss 0.23202609450090678\n",
      "error 224419 CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 4.00 GiB total capacity; 2.46 GiB already allocated; 64.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 225000 loss 0.2256322847371921\n",
      "step 226000 loss 0.2333413270013407\n",
      "step 227000 loss 0.23235008171200752\n",
      "step 228000 loss 0.23773303290922196\n",
      "error 228283 CUDA out of memory. Tried to allocate 176.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 168.20 MiB free; 2.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 229000 loss 0.22254937118571252\n",
      "step 230000 loss 0.23649040507571772\n",
      "step 231000 loss 0.21750179457152263\n",
      "step 232000 loss 0.2253675498859957\n",
      "step 233000 loss 0.23385746720153838\n",
      "step 234000 loss 0.2282316687256098\n",
      "error 234376 CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 4.00 GiB total capacity; 2.44 GiB already allocated; 60.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 235000 loss 0.21989145450945943\n",
      "step 236000 loss 0.21367947154305875\n",
      "step 237000 loss 0.22754633634164928\n",
      "step 238000 loss 0.20981563916243612\n",
      "step 239000 loss 0.21510113241011278\n",
      "step 240000 loss 0.21446957120439036\n",
      "step 241000 loss 0.2409097735118121\n",
      "error 241212 CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 4.00 GiB total capacity; 2.57 GiB already allocated; 8.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 242000 loss 0.22253383105248212\n",
      "step 243000 loss 0.2150265941056423\n",
      "step 244000 loss 0.21536659738607705\n",
      "step 245000 loss 0.2127732679513283\n",
      "step 246000 loss 0.21855682098586113\n",
      "step 247000 loss 0.21643042163783685\n",
      "step 248000 loss 0.2070042179338634\n",
      "error 248673 CUDA out of memory. Tried to allocate 122.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 62.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 249000 loss 0.21502306727552786\n",
      "step 250000 loss 0.21707071574870496\n",
      "error 250815 CUDA out of memory. Tried to allocate 480.00 MiB (GPU 0; 4.00 GiB total capacity; 1.96 GiB already allocated; 308.20 MiB free; 2.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 251000 loss 0.20810007807193323\n",
      "step 252000 loss 0.21283295540697872\n",
      "error 252321 CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 4.00 GiB total capacity; 2.57 GiB already allocated; 32.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 253000 loss 0.21498956191912294\n",
      "error 253240 CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 2.57 GiB already allocated; 204.80 KiB free; 2.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 254000 loss 0.21005698555894195\n",
      "step 255000 loss 0.1991607239590958\n",
      "step 256000 loss 0.21100708231190218\n",
      "step 257000 loss 0.21461173384310678\n",
      "step 258000 loss 0.2182565502151847\n",
      "error 258875 CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 4.00 GiB total capacity; 2.55 GiB already allocated; 70.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 259000 loss 0.21043755669612438\n",
      "step 260000 loss 0.2040488691176288\n",
      "step 261000 loss 0.21510407200921328\n",
      "step 262000 loss 0.2185553268524818\n",
      "step 263000 loss 0.20283651127293706\n",
      "step 264000 loss 0.19573056991724297\n",
      "step 265000 loss 0.20161180541757495\n",
      "step 266000 loss 0.2086902159466408\n",
      "step 267000 loss 0.1966990050910972\n",
      "step 268000 loss 0.1967500068549998\n",
      "step 269000 loss 0.2097488444596529\n",
      "step 270000 loss 0.20010407420340925\n",
      "step 271000 loss 0.21253989042853935\n",
      "step 272000 loss 0.2030993716288358\n",
      "step 273000 loss 0.19222208619769662\n",
      "step 274000 loss 0.2003104875064455\n",
      "step 275000 loss 0.20553905482566914\n",
      "error 275042 CUDA out of memory. Tried to allocate 492.00 MiB (GPU 0; 4.00 GiB total capacity; 2.17 GiB already allocated; 440.20 MiB free; 2.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 276000 loss 0.20355474954424427\n",
      "step 277000 loss 0.21121875181887298\n",
      "step 278000 loss 0.20273590342979877\n",
      "error 278889 CUDA out of memory. Tried to allocate 134.00 MiB (GPU 0; 4.00 GiB total capacity; 2.35 GiB already allocated; 98.20 MiB free; 2.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 279000 loss 0.18652552053006366\n",
      "step 280000 loss 0.20719787701638415\n",
      "step 281000 loss 0.19839643052313477\n",
      "error 281322 CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 4.00 GiB total capacity; 2.50 GiB already allocated; 96.20 MiB free; 2.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 282000 loss 0.19387951805023476\n",
      "step 283000 loss 0.1911156460493803\n",
      "step 284000 loss 0.20163044184958562\n",
      "step 285000 loss 0.19399882914125918\n",
      "step 286000 loss 0.19123696274124086\n",
      "step 287000 loss 0.19180777393979953\n",
      "step 288000 loss 0.1890208238377236\n",
      "step 289000 loss 0.19855932352598757\n",
      "error 289290 CUDA out of memory. Tried to allocate 508.00 MiB (GPU 0; 4.00 GiB total capacity; 1.93 GiB already allocated; 236.20 MiB free; 2.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 289696 CUDA out of memory. Tried to allocate 448.00 MiB (GPU 0; 4.00 GiB total capacity; 1.98 GiB already allocated; 236.20 MiB free; 2.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 290000 loss 0.19761979434639215\n",
      "step 291000 loss 0.18509383201878518\n",
      "step 292000 loss 0.1890546025731601\n",
      "step 293000 loss 0.18244791504647584\n",
      "error 293204 CUDA out of memory. Tried to allocate 254.00 MiB (GPU 0; 4.00 GiB total capacity; 2.01 GiB already allocated; 230.20 MiB free; 2.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 294000 loss 0.19011717846337706\n",
      "step 295000 loss 0.18336256873607634\n",
      "step 296000 loss 0.1932023362973705\n",
      "step 297000 loss 0.18591941213281826\n",
      "error 297304 CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 4.00 GiB total capacity; 2.24 GiB already allocated; 100.20 MiB free; 2.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 298000 loss 0.1866568825817667\n",
      "step 299000 loss 0.1992029502163641\n",
      "error 299033 CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 4.00 GiB total capacity; 2.47 GiB already allocated; 82.20 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 300000 loss 0.17793840881856157\n",
      "step 301000 loss 0.19116759006120265\n",
      "step 302000 loss 0.19774619738548063\n",
      "step 303000 loss 0.18101940819667653\n",
      "step 304000 loss 0.181496234646067\n",
      "step 305000 loss 0.18439561044657604\n",
      "step 306000 loss 0.1869864042950794\n",
      "error 306398 CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 4.00 GiB total capacity; 2.38 GiB already allocated; 20.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 307000 loss 0.1796159782060422\n",
      "error 307636 CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 4.00 GiB total capacity; 2.54 GiB already allocated; 40.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 308000 loss 0.18122519653500058\n",
      "step 309000 loss 0.18926862568082287\n",
      "step 310000 loss 0.18545510611962526\n",
      "step 311000 loss 0.17857523324107752\n",
      "error 311047 CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 4.00 GiB total capacity; 2.55 GiB already allocated; 24.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 312000 loss 0.18320936574600638\n",
      "step 313000 loss 0.182819615974091\n",
      "step 314000 loss 0.1869017479699105\n",
      "step 315000 loss 0.1828177864353638\n",
      "step 316000 loss 0.1703542187823914\n",
      "step 317000 loss 0.1790263232169673\n",
      "step 318000 loss 0.16722141934372484\n",
      "step 319000 loss 0.17857712321262806\n",
      "step 320000 loss 0.17427900187624618\n",
      "error 320974 CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 4.00 GiB total capacity; 2.53 GiB already allocated; 58.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 321000 loss 0.1798450035895221\n",
      "step 322000 loss 0.17408271969808264\n",
      "step 323000 loss 0.17232216107239948\n",
      "step 324000 loss 0.1757009950648062\n",
      "step 325000 loss 0.17314720792649313\n",
      "error 325748 CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 4.00 GiB total capacity; 2.47 GiB already allocated; 68.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 326000 loss 0.18664650266850366\n",
      "step 327000 loss 0.168217349792365\n",
      "step 328000 loss 0.17820850832248106\n",
      "error 328598 CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 4.00 GiB total capacity; 2.58 GiB already allocated; 38.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 328636 CUDA out of memory. Tried to allocate 88.00 MiB (GPU 0; 4.00 GiB total capacity; 2.55 GiB already allocated; 66.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 329000 loss 0.17743107713665812\n",
      "step 330000 loss 0.16849885499058292\n",
      "step 331000 loss 0.17633997673587873\n",
      "error 331431 CUDA out of memory. Tried to allocate 590.00 MiB (GPU 0; 4.00 GiB total capacity; 2.22 GiB already allocated; 350.20 MiB free; 2.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 332000 loss 0.16692381029995157\n",
      "error 332722 CUDA out of memory. Tried to allocate 472.00 MiB (GPU 0; 4.00 GiB total capacity; 1.80 GiB already allocated; 286.20 MiB free; 2.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 333000 loss 0.18044143824512138\n",
      "error 333412 CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 4.00 GiB total capacity; 2.54 GiB already allocated; 204.80 KiB free; 2.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 333628 CUDA out of memory. Tried to allocate 174.00 MiB (GPU 0; 4.00 GiB total capacity; 2.58 GiB already allocated; 34.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 334000 loss 0.18291757403220982\n",
      "step 335000 loss 0.16768302125157789\n",
      "step 336000 loss 0.18590462415665387\n",
      "step 337000 loss 0.17024390653846785\n",
      "step 338000 loss 0.17415707915811798\n",
      "step 339000 loss 0.18166826070379466\n",
      "error 339440 CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 4.00 GiB total capacity; 2.34 GiB already allocated; 156.20 MiB free; 2.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 340000 loss 0.1645725715351291\n",
      "step 341000 loss 0.1753199485936202\n",
      "step 342000 loss 0.1774589260709472\n",
      "error 342260 CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 4.00 GiB total capacity; 2.08 GiB already allocated; 56.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 343000 loss 0.16828095806529744\n",
      "step 344000 loss 0.16716801011608914\n",
      "step 345000 loss 0.15690677742147818\n",
      "step 346000 loss 0.16415140146622434\n",
      "step 347000 loss 0.15129991651000455\n",
      "step 348000 loss 0.16238008170947432\n",
      "error 348891 CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 4.00 GiB total capacity; 2.54 GiB already allocated; 48.20 MiB free; 2.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 349000 loss 0.1835913309790194\n",
      "step 350000 loss 0.16385906537808478\n",
      "step 351000 loss 0.15863240622309968\n",
      "step 352000 loss 0.16448412910779006\n",
      "step 353000 loss 0.16858733404427767\n",
      "step 354000 loss 0.1615497427242808\n",
      "error 354310 CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 4.00 GiB total capacity; 2.56 GiB already allocated; 34.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 355000 loss 0.16630374418012797\n",
      "error 355233 CUDA out of memory. Tried to allocate 334.00 MiB (GPU 0; 4.00 GiB total capacity; 2.26 GiB already allocated; 66.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 356000 loss 0.16670519574638457\n",
      "step 357000 loss 0.16820591592043638\n",
      "step 358000 loss 0.16525786286685615\n",
      "step 359000 loss 0.16477733510686085\n",
      "step 360000 loss 0.15813801798364147\n",
      "step 361000 loss 0.15539632989512756\n",
      "step 362000 loss 0.16185182839166373\n",
      "step 363000 loss 0.16325885132537224\n",
      "step 364000 loss 0.15463007974717766\n",
      "step 365000 loss 0.15590744015993552\n",
      "step 366000 loss 0.1607919734972529\n",
      "error 366652 CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 4.00 GiB total capacity; 2.48 GiB already allocated; 56.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 367000 loss 0.15429479070100932\n",
      "step 368000 loss 0.15923043166846038\n",
      "step 369000 loss 0.1575234336233698\n",
      "step 370000 loss 0.15588050352176652\n",
      "step 371000 loss 0.16407334075076505\n",
      "step 372000 loss 0.15199075029976666\n",
      "step 373000 loss 0.16783443080913277\n",
      "step 374000 loss 0.1588276833575219\n",
      "step 375000 loss 0.15078909955220296\n",
      "step 376000 loss 0.15468736288417131\n",
      "error 376070 CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 72.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 377000 loss 0.15958125275606289\n",
      "step 378000 loss 0.15759516025753692\n",
      "step 379000 loss 0.1612064256677404\n",
      "step 380000 loss 0.14950356920063496\n",
      "step 381000 loss 0.15988918294897303\n",
      "step 382000 loss 0.16023260991787538\n",
      "error 382939 CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 4.00 GiB total capacity; 2.51 GiB already allocated; 76.20 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 383000 loss 0.16300548178981988\n",
      "step 384000 loss 0.15121915566315874\n",
      "error 384372 CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 4.00 GiB total capacity; 2.57 GiB already allocated; 28.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 385000 loss 0.15283978592185304\n",
      "step 386000 loss 0.16279121616738848\n",
      "error 386780 CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 4.00 GiB total capacity; 2.48 GiB already allocated; 84.20 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 387000 loss 0.15155416259681806\n",
      "error 387168 CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 4.00 GiB total capacity; 2.51 GiB already allocated; 42.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 388000 loss 0.1633849442829378\n",
      "step 389000 loss 0.15306160370493307\n",
      "step 390000 loss 0.15441786128422244\n",
      "step 391000 loss 0.15263255557604133\n",
      "step 392000 loss 0.16108612666791305\n",
      "step 393000 loss 0.16335776388598605\n",
      "step 394000 loss 0.14685285579506308\n",
      "step 395000 loss 0.15100088923447766\n",
      "step 396000 loss 0.15496694307704456\n",
      "step 397000 loss 0.15791028853016906\n",
      "step 398000 loss 0.1588789056604728\n",
      "step 399000 loss 0.15382270171726123\n",
      "error 399869 CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 4.00 GiB total capacity; 2.57 GiB already allocated; 36.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 400000 loss 0.1549449942256324\n",
      "error 400336 CUDA out of memory. Tried to allocate 460.00 MiB (GPU 0; 4.00 GiB total capacity; 2.43 GiB already allocated; 190.20 MiB free; 2.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 401000 loss 0.15953567184647546\n",
      "error 401079 CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 4.00 GiB total capacity; 2.38 GiB already allocated; 64.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 402000 loss 0.14984670870238914\n",
      "step 403000 loss 0.1433471206361428\n",
      "step 404000 loss 0.14926939634629524\n",
      "step 405000 loss 0.14935343000292778\n",
      "step 406000 loss 0.14264844172680752\n",
      "step 407000 loss 0.15047082553850488\n",
      "step 408000 loss 0.14868339111981912\n",
      "step 409000 loss 0.14978007112606428\n",
      "error 409542 CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.60 GiB already allocated; 14.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 410000 loss 0.15697440142347477\n",
      "step 411000 loss 0.15245422380417586\n",
      "step 412000 loss 0.14209587806276977\n",
      "step 413000 loss 0.14475336105795578\n",
      "error 413674 CUDA out of memory. Tried to allocate 122.00 MiB (GPU 0; 4.00 GiB total capacity; 2.47 GiB already allocated; 90.20 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 414000 loss 0.14778271145932376\n",
      "step 415000 loss 0.14803185919998213\n",
      "step 416000 loss 0.14841456333640962\n",
      "step 417000 loss 0.14584862525155767\n",
      "step 418000 loss 0.14736333008529617\n",
      "step 419000 loss 0.14608943263045512\n",
      "step 420000 loss 0.14228078519785778\n",
      "step 421000 loss 0.1445746573009528\n",
      "step 422000 loss 0.14381155288801528\n",
      "step 423000 loss 0.1424932270140853\n",
      "step 424000 loss 0.1497580757937394\n",
      "step 425000 loss 0.15207350585609675\n",
      "step 426000 loss 0.14545215648273005\n",
      "step 427000 loss 0.14314646201976575\n",
      "step 428000 loss 0.14411613397393375\n",
      "step 429000 loss 0.13471246169530787\n",
      "step 430000 loss 0.14369090172764845\n",
      "step 431000 loss 0.14531493367673828\n",
      "step 432000 loss 0.14051133470609783\n",
      "step 433000 loss 0.14390786898182706\n",
      "step 434000 loss 0.15123749153735117\n",
      "step 435000 loss 0.1374214528852608\n",
      "step 436000 loss 0.1433770820661448\n",
      "error 436410 CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 4.00 GiB total capacity; 2.37 GiB already allocated; 248.20 MiB free; 2.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 437000 loss 0.14178635563491843\n",
      "step 438000 loss 0.14953296012571082\n",
      "error 438531 CUDA out of memory. Tried to allocate 138.00 MiB (GPU 0; 4.00 GiB total capacity; 2.46 GiB already allocated; 106.20 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 439000 loss 0.13452935010846703\n",
      "error 439411 CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 4.00 GiB total capacity; 2.50 GiB already allocated; 6.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 440000 loss 0.14626205102796666\n",
      "step 441000 loss 0.14189740792522207\n",
      "error 441094 CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 4.00 GiB total capacity; 2.59 GiB already allocated; 4.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 442000 loss 0.1421364339189604\n",
      "step 443000 loss 0.14776995412330143\n",
      "error 443714 CUDA out of memory. Tried to allocate 314.00 MiB (GPU 0; 4.00 GiB total capacity; 2.13 GiB already allocated; 218.20 MiB free; 2.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 444000 loss 0.14259000100614502\n",
      "step 445000 loss 0.14290770666487515\n",
      "step 446000 loss 0.15027385524846612\n",
      "step 447000 loss 0.14328342629689722\n",
      "step 448000 loss 0.15046023918921128\n",
      "error 448995 CUDA out of memory. Tried to allocate 246.00 MiB (GPU 0; 4.00 GiB total capacity; 2.26 GiB already allocated; 184.20 MiB free; 2.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 449000 loss 0.13740931453974917\n",
      "step 450000 loss 0.14129088634927758\n",
      "error 450056 CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 2.56 GiB already allocated; 4.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 450394 CUDA out of memory. Tried to allocate 126.00 MiB (GPU 0; 4.00 GiB total capacity; 2.55 GiB already allocated; 20.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 451000 loss 0.13689366140938364\n",
      "step 452000 loss 0.13486919765779748\n",
      "error 452263 CUDA out of memory. Tried to allocate 320.00 MiB (GPU 0; 4.00 GiB total capacity; 2.16 GiB already allocated; 170.20 MiB free; 2.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 453000 loss 0.1330342805115506\n",
      "step 454000 loss 0.1405926973107271\n",
      "step 455000 loss 0.1304072535405867\n",
      "step 456000 loss 0.14009099586727097\n",
      "step 457000 loss 0.13604291272582486\n",
      "step 458000 loss 0.1345101175091695\n",
      "step 459000 loss 0.13699141068011522\n",
      "step 460000 loss 0.14397532779490574\n",
      "step 461000 loss 0.1334154978885781\n",
      "step 462000 loss 0.12605381245701575\n",
      "step 463000 loss 0.13490735336369836\n",
      "step 464000 loss 0.13657157248770818\n",
      "error 464744 CUDA out of memory. Tried to allocate 514.00 MiB (GPU 0; 4.00 GiB total capacity; 2.08 GiB already allocated; 150.20 MiB free; 2.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 465000 loss 0.13461953924130649\n",
      "step 466000 loss 0.13400295914383606\n",
      "error 466202 CUDA out of memory. Tried to allocate 2.92 GiB (GPU 0; 4.00 GiB total capacity; 220.35 MiB already allocated; 2.32 GiB free; 332.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 467000 loss 0.13300192033383065\n",
      "step 468000 loss 0.13056354971276596\n",
      "step 469000 loss 0.1368791932889726\n",
      "step 470000 loss 0.12903902879403903\n",
      "step 471000 loss 0.13363329035369678\n",
      "step 472000 loss 0.13011479379655794\n",
      "error 472488 CUDA out of memory. Tried to allocate 184.00 MiB (GPU 0; 4.00 GiB total capacity; 2.54 GiB already allocated; 68.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 473000 loss 0.13705525900935756\n",
      "step 474000 loss 0.13492263778857888\n",
      "step 475000 loss 0.12432954806508496\n",
      "step 476000 loss 0.1332178042291198\n",
      "error 476326 CUDA out of memory. Tried to allocate 554.00 MiB (GPU 0; 4.00 GiB total capacity; 2.09 GiB already allocated; 526.20 MiB free; 2.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 476380 CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 4.00 GiB total capacity; 2.54 GiB already allocated; 28.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 477000 loss 0.14571343660866842\n",
      "step 478000 loss 0.1310203411327675\n",
      "step 479000 loss 0.13294546341430397\n",
      "step 480000 loss 0.1349485713019967\n",
      "step 481000 loss 0.1376292708395049\n",
      "error 481398 CUDA out of memory. Tried to allocate 644.00 MiB (GPU 0; 4.00 GiB total capacity; 2.08 GiB already allocated; 550.20 MiB free; 2.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 482000 loss 0.13686784268543123\n",
      "step 483000 loss 0.12929512061434797\n",
      "step 484000 loss 0.12547385819884949\n",
      "error 484047 CUDA out of memory. Tried to allocate 574.00 MiB (GPU 0; 4.00 GiB total capacity; 2.16 GiB already allocated; 8.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 485000 loss 0.1325635314406827\n",
      "error 485472 CUDA out of memory. Tried to allocate 510.00 MiB (GPU 0; 4.00 GiB total capacity; 2.24 GiB already allocated; 374.20 MiB free; 2.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 486000 loss 0.12151867371448316\n",
      "step 487000 loss 0.11601565862656571\n",
      "error 487363 CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 4.00 GiB total capacity; 2.57 GiB already allocated; 54.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 488000 loss 0.1251782844790723\n",
      "step 489000 loss 0.1372545051558409\n",
      "step 490000 loss 0.12674162138765677\n",
      "step 491000 loss 0.1216040655951947\n",
      "error 491946 CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 4.00 GiB total capacity; 2.58 GiB already allocated; 12.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 492000 loss 0.1332873311918229\n",
      "error 492215 CUDA out of memory. Tried to allocate 184.00 MiB (GPU 0; 4.00 GiB total capacity; 2.49 GiB already allocated; 140.20 MiB free; 2.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 493000 loss 0.12505440906109289\n",
      "step 494000 loss 0.13690396101679653\n",
      "step 495000 loss 0.13222436448000371\n",
      "step 496000 loss 0.13064532411051913\n",
      "step 497000 loss 0.1280140711404383\n",
      "step 498000 loss 0.12866014077421278\n",
      "step 499000 loss 0.13330923733883537\n",
      "error 499014 CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 164.20 MiB free; 2.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 500000 loss 0.13381281646550633\n",
      "step 501000 loss 0.1298377354675904\n",
      "step 502000 loss 0.1403309170268476\n",
      "error 502317 CUDA out of memory. Tried to allocate 258.00 MiB (GPU 0; 4.00 GiB total capacity; 2.21 GiB already allocated; 194.20 MiB free; 2.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 503000 loss 0.12782926344661974\n",
      "error 503102 CUDA out of memory. Tried to allocate 138.00 MiB (GPU 0; 4.00 GiB total capacity; 2.46 GiB already allocated; 132.20 MiB free; 2.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 503386 CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 4.00 GiB total capacity; 2.47 GiB already allocated; 114.20 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 504000 loss 0.12878803132125177\n",
      "step 505000 loss 0.13129009760147892\n",
      "step 506000 loss 0.12589322537893896\n",
      "step 507000 loss 0.12878754936996847\n",
      "error 507475 CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 176.20 MiB free; 2.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 508000 loss 0.12955787990009413\n",
      "step 509000 loss 0.13401356011605822\n",
      "step 510000 loss 0.1309831263711676\n",
      "step 511000 loss 0.12052694856165909\n",
      "step 512000 loss 0.1225209368225187\n",
      "error 512993 CUDA out of memory. Tried to allocate 236.00 MiB (GPU 0; 4.00 GiB total capacity; 2.17 GiB already allocated; 222.20 MiB free; 2.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 513000 loss 0.12551147224474699\n",
      "step 514000 loss 0.12328751448937691\n",
      "step 515000 loss 0.12693971003079788\n",
      "error 515046 CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 4.00 GiB total capacity; 2.47 GiB already allocated; 20.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 516000 loss 0.12767111679865048\n",
      "step 517000 loss 0.12252364782523364\n",
      "error 517812 CUDA out of memory. Tried to allocate 612.00 MiB (GPU 0; 4.00 GiB total capacity; 2.30 GiB already allocated; 282.20 MiB free; 2.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 518000 loss 0.13224996656924487\n",
      "step 519000 loss 0.11819896231568418\n",
      "step 520000 loss 0.13260920884995722\n",
      "step 521000 loss 0.1300664225546643\n",
      "step 522000 loss 0.12409477728419006\n",
      "error 522673 CUDA out of memory. Tried to allocate 178.00 MiB (GPU 0; 4.00 GiB total capacity; 2.40 GiB already allocated; 122.20 MiB free; 2.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 523000 loss 0.11990640154131688\n",
      "step 524000 loss 0.12599558843113481\n",
      "step 525000 loss 0.12587471819599158\n",
      "step 526000 loss 0.12636624379153363\n",
      "error 526070 CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 4.00 GiB total capacity; 2.46 GiB already allocated; 10.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 527000 loss 0.12595838737790474\n",
      "error 527235 CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 4.00 GiB total capacity; 2.60 GiB already allocated; 204.80 KiB free; 2.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 528000 loss 0.12825643426110037\n",
      "step 529000 loss 0.11702221319661475\n",
      "step 530000 loss 0.12305468062590807\n",
      "error 530749 CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 156.20 MiB free; 2.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 531000 loss 0.11906997286947445\n",
      "step 532000 loss 0.1201627618346829\n",
      "step 533000 loss 0.12045393923297525\n",
      "step 534000 loss 0.1240807672557421\n",
      "step 535000 loss 0.12471826008195057\n",
      "step 536000 loss 0.1300738517146092\n",
      "step 537000 loss 0.12099332401505672\n",
      "step 538000 loss 0.1268121346323751\n",
      "error 538909 CUDA out of memory. Tried to allocate 156.00 MiB (GPU 0; 4.00 GiB total capacity; 2.32 GiB already allocated; 150.20 MiB free; 2.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 539000 loss 0.12429434962524101\n",
      "step 540000 loss 0.12363705953885801\n",
      "error 540757 CUDA out of memory. Tried to allocate 214.00 MiB (GPU 0; 4.00 GiB total capacity; 1.85 GiB already allocated; 180.20 MiB free; 2.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 541000 loss 0.11963403317285702\n",
      "step 542000 loss 0.12126318018743769\n",
      "step 543000 loss 0.1210788367819041\n",
      "step 544000 loss 0.11314609308354556\n",
      "step 545000 loss 0.11939321056543849\n",
      "error 545874 CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 4.00 GiB total capacity; 2.01 GiB already allocated; 180.20 MiB free; 2.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 546000 loss 0.12558474540105088\n",
      "step 547000 loss 0.11896669830847531\n",
      "step 548000 loss 0.12017571224272251\n",
      "step 549000 loss 0.11600011395057663\n",
      "step 550000 loss 0.11953576495195739\n",
      "step 551000 loss 0.11852424774272367\n",
      "error 551121 CUDA out of memory. Tried to allocate 140.00 MiB (GPU 0; 4.00 GiB total capacity; 2.49 GiB already allocated; 32.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 552000 loss 0.1233181837273296\n",
      "error 552067 CUDA out of memory. Tried to allocate 206.00 MiB (GPU 0; 4.00 GiB total capacity; 2.47 GiB already allocated; 86.20 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 552457 CUDA out of memory. Tried to allocate 174.00 MiB (GPU 0; 4.00 GiB total capacity; 2.58 GiB already allocated; 2.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 553000 loss 0.12856424866768065\n",
      "step 554000 loss 0.11407253621821292\n",
      "error 554214 CUDA out of memory. Tried to allocate 2.93 GiB (GPU 0; 4.00 GiB total capacity; 222.99 MiB already allocated; 2.38 GiB free; 270.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 554551 CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 180.20 MiB free; 2.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 554955 CUDA out of memory. Tried to allocate 252.00 MiB (GPU 0; 4.00 GiB total capacity; 2.15 GiB already allocated; 216.20 MiB free; 2.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 555000 loss 0.11791268126480281\n",
      "error 555362 CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 4.00 GiB total capacity; 2.10 GiB already allocated; 224.20 MiB free; 2.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 556000 loss 0.1201929741750937\n",
      "error 556719 CUDA out of memory. Tried to allocate 334.00 MiB (GPU 0; 4.00 GiB total capacity; 2.26 GiB already allocated; 90.20 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 557000 loss 0.12382616680231877\n",
      "error 557159 CUDA out of memory. Tried to allocate 178.00 MiB (GPU 0; 4.00 GiB total capacity; 2.41 GiB already allocated; 80.20 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 558000 loss 0.11927361842989921\n",
      "error 558690 CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 4.00 GiB total capacity; 2.48 GiB already allocated; 44.20 MiB free; 2.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 559000 loss 0.1150800149468705\n",
      "step 560000 loss 0.12144295880710707\n",
      "step 561000 loss 0.11096268541831524\n",
      "step 562000 loss 0.1243954921120312\n",
      "step 563000 loss 0.1273931681246031\n",
      "step 564000 loss 0.11630318406154401\n",
      "step 565000 loss 0.11971781189972534\n",
      "step 566000 loss 0.12106155532598495\n",
      "step 567000 loss 0.11285549202375114\n",
      "error 567007 CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 4.00 GiB total capacity; 2.32 GiB already allocated; 280.20 MiB free; 2.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 568000 loss 0.11179238700284623\n",
      "step 569000 loss 0.11574930018861779\n",
      "step 570000 loss 0.11254199402779341\n",
      "step 571000 loss 0.11300373737385962\n",
      "error 571574 CUDA out of memory. Tried to allocate 116.00 MiB (GPU 0; 4.00 GiB total capacity; 2.50 GiB already allocated; 36.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 572000 loss 0.1186899132640101\n",
      "step 573000 loss 0.12370520810084418\n",
      "error 573220 CUDA out of memory. Tried to allocate 106.00 MiB (GPU 0; 4.00 GiB total capacity; 2.54 GiB already allocated; 2.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 574000 loss 0.11594963684328832\n",
      "step 575000 loss 0.11226616527535953\n",
      "step 576000 loss 0.11612451972416601\n",
      "step 577000 loss 0.116571980540175\n",
      "error 577144 CUDA out of memory. Tried to allocate 170.00 MiB (GPU 0; 4.00 GiB total capacity; 2.51 GiB already allocated; 14.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 578000 loss 0.11310610706056468\n",
      "step 579000 loss 0.11990106435702183\n",
      "error 579754 CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 4.00 GiB total capacity; 2.48 GiB already allocated; 54.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 580000 loss 0.11783505863905884\n",
      "step 581000 loss 0.11409533483581617\n",
      "step 582000 loss 0.12396441661822609\n",
      "step 583000 loss 0.12142795071518049\n",
      "step 584000 loss 0.11241227007750422\n",
      "step 585000 loss 0.10932547834096476\n",
      "error 585689 CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 4.00 GiB total capacity; 2.57 GiB already allocated; 36.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 586000 loss 0.10678707021102309\n",
      "error 586837 CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 4.00 GiB total capacity; 2.55 GiB already allocated; 70.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 587000 loss 0.11814660789910704\n",
      "step 588000 loss 0.11347663921630011\n",
      "step 589000 loss 0.10804439216689207\n",
      "step 590000 loss 0.12020849046320654\n",
      "step 591000 loss 0.11598338828282431\n",
      "step 592000 loss 0.12110400974936783\n",
      "error 592268 CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 4.00 GiB total capacity; 2.54 GiB already allocated; 32.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 592536 CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 4.00 GiB total capacity; 2.25 GiB already allocated; 184.20 MiB free; 2.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 593000 loss 0.11335973016521893\n",
      "step 594000 loss 0.11563629789231344\n",
      "step 595000 loss 0.1125190620860085\n",
      "step 596000 loss 0.10816905600484461\n",
      "step 597000 loss 0.10920356335863471\n",
      "error 597633 CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 2.57 GiB already allocated; 28.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 598000 loss 0.11315828039858025\n",
      "step 599000 loss 0.11150066333217547\n",
      "step 600000 loss 0.1143283031177707\n",
      "step 601000 loss 0.11444567625015042\n",
      "step 602000 loss 0.11430175110581331\n",
      "step 603000 loss 0.11719801819580607\n",
      "step 604000 loss 0.11695453330874443\n",
      "error 604977 CUDA out of memory. Tried to allocate 200.00 MiB (GPU 0; 4.00 GiB total capacity; 2.48 GiB already allocated; 20.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 605000 loss 0.11410022415965795\n",
      "step 606000 loss 0.1125775497672148\n",
      "step 607000 loss 0.11312303301517386\n",
      "error 607330 CUDA out of memory. Tried to allocate 4.69 GiB (GPU 0; 4.00 GiB total capacity; 249.55 MiB already allocated; 2.31 GiB free; 340.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 608000 loss 0.11633160413661972\n",
      "step 609000 loss 0.1132998137997929\n",
      "step 610000 loss 0.1137672165920958\n",
      "error 610119 CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 4.00 GiB total capacity; 2.53 GiB already allocated; 80.20 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 611000 loss 0.1161630370519124\n",
      "step 612000 loss 0.11210748910577968\n",
      "step 613000 loss 0.11527370602427982\n",
      "step 614000 loss 0.10996631121402606\n",
      "step 615000 loss 0.1103725339663215\n",
      "step 616000 loss 0.1155465511905495\n",
      "step 617000 loss 0.10843856428074651\n",
      "step 618000 loss 0.11500903404830023\n",
      "step 619000 loss 0.10631863138405606\n",
      "step 620000 loss 0.11313636640110053\n",
      "step 621000 loss 0.10649769437359646\n",
      "step 622000 loss 0.11899125240836292\n",
      "step 623000 loss 0.1107739432414528\n",
      "step 624000 loss 0.11182427258044482\n",
      "step 625000 loss 0.1169732006371487\n",
      "step 626000 loss 0.11296796128759161\n",
      "step 627000 loss 0.11532149268989451\n",
      "error 627092 CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 4.00 GiB total capacity; 2.48 GiB already allocated; 74.20 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 628000 loss 0.11790422331192531\n",
      "step 629000 loss 0.10922682221187278\n",
      "step 630000 loss 0.11183417933620513\n",
      "step 631000 loss 0.1103920483503025\n",
      "step 632000 loss 0.10869552431395277\n",
      "step 633000 loss 0.10734511881228537\n",
      "step 634000 loss 0.10651082758745178\n",
      "step 635000 loss 0.11064377554273233\n",
      "step 636000 loss 0.10535105924960225\n",
      "step 637000 loss 0.1099833263119217\n",
      "step 638000 loss 0.1084320371421054\n",
      "step 639000 loss 0.11008111399342306\n",
      "error 639635 CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 4.00 GiB total capacity; 2.34 GiB already allocated; 12.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 640000 loss 0.10387712128972634\n",
      "step 641000 loss 0.11719345789495855\n",
      "error 641942 CUDA out of memory. Tried to allocate 306.00 MiB (GPU 0; 4.00 GiB total capacity; 2.58 GiB already allocated; 6.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 642000 loss 0.11218127903994173\n",
      "step 643000 loss 0.1107912263199687\n",
      "error 643828 CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 4.00 GiB total capacity; 2.58 GiB already allocated; 38.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 644000 loss 0.1065028185765259\n",
      "step 645000 loss 0.10467859716597014\n",
      "error 645398 CUDA out of memory. Tried to allocate 4.38 GiB (GPU 0; 4.00 GiB total capacity; 246.58 MiB already allocated; 2.34 GiB free; 318.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 646000 loss 0.11199707807204685\n",
      "step 647000 loss 0.11511536850151606\n",
      "step 648000 loss 0.1061918785229791\n",
      "step 649000 loss 0.10646212750230916\n",
      "step 650000 loss 0.11056248764321208\n",
      "error 650165 CUDA out of memory. Tried to allocate 422.00 MiB (GPU 0; 4.00 GiB total capacity; 2.15 GiB already allocated; 142.20 MiB free; 2.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 650765 CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 4.00 GiB total capacity; 2.11 GiB already allocated; 280.20 MiB free; 2.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 651000 loss 0.10677461722120643\n",
      "step 652000 loss 0.10664921226352453\n",
      "error 652051 CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 4.00 GiB total capacity; 2.43 GiB already allocated; 88.20 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 653000 loss 0.10927609609015053\n",
      "step 654000 loss 0.105909581087064\n",
      "step 655000 loss 0.11102467348671052\n",
      "step 656000 loss 0.1097198750267271\n",
      "step 657000 loss 0.10322914730967023\n",
      "error 657748 CUDA out of memory. Tried to allocate 264.00 MiB (GPU 0; 4.00 GiB total capacity; 2.40 GiB already allocated; 156.20 MiB free; 2.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 658000 loss 0.10163522344990633\n",
      "step 659000 loss 0.1023579938521143\n",
      "step 660000 loss 0.0977246198700741\n",
      "step 661000 loss 0.10864220717665739\n",
      "step 662000 loss 0.1113071688353084\n",
      "step 663000 loss 0.1114467758901883\n",
      "error 663029 CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 4.00 GiB total capacity; 2.35 GiB already allocated; 36.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 664000 loss 0.1072290889865253\n",
      "step 665000 loss 0.1127061782174278\n",
      "step 666000 loss 0.11038534102798439\n",
      "step 667000 loss 0.10960639730957336\n",
      "step 668000 loss 0.10777082089637406\n",
      "step 669000 loss 0.10692843252490275\n",
      "step 670000 loss 0.11550479167257435\n",
      "step 671000 loss 0.10270967293530703\n",
      "step 672000 loss 0.10824718704260886\n",
      "error 672061 CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 4.00 GiB total capacity; 2.48 GiB already allocated; 86.20 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 672332 CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 4.00 GiB total capacity; 2.57 GiB already allocated; 36.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 673000 loss 0.10947341004549525\n",
      "step 674000 loss 0.10383020505099558\n",
      "step 675000 loss 0.10942471825424582\n",
      "step 676000 loss 0.11076417814334855\n",
      "step 677000 loss 0.10671308600273914\n",
      "step 678000 loss 0.10803068639466074\n",
      "step 679000 loss 0.10434366384567693\n",
      "error 679871 CUDA out of memory. Tried to allocate 304.00 MiB (GPU 0; 4.00 GiB total capacity; 2.37 GiB already allocated; 234.20 MiB free; 2.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 680000 loss 0.10018078953935765\n",
      "step 681000 loss 0.10599166701501235\n",
      "step 682000 loss 0.11265642709913663\n",
      "error 682199 CUDA out of memory. Tried to allocate 424.00 MiB (GPU 0; 4.00 GiB total capacity; 2.16 GiB already allocated; 134.20 MiB free; 2.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 683000 loss 0.10334436303121038\n",
      "step 684000 loss 0.09936379901459441\n",
      "step 685000 loss 0.10416389326634817\n",
      "step 686000 loss 0.10324967895960435\n",
      "step 687000 loss 0.10191476429486647\n",
      "error 687060 CUDA out of memory. Tried to allocate 8.87 GiB (GPU 0; 4.00 GiB total capacity; 302.05 MiB already allocated; 2.24 GiB free; 414.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 688000 loss 0.10611859543621541\n",
      "step 689000 loss 0.10271917479857802\n",
      "step 690000 loss 0.11013093890319578\n",
      "step 691000 loss 0.11076286906423047\n",
      "step 692000 loss 0.11251740445941687\n",
      "step 693000 loss 0.11722669478156604\n",
      "error 693549 CUDA out of memory. Tried to allocate 240.00 MiB (GPU 0; 4.00 GiB total capacity; 2.30 GiB already allocated; 104.20 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 694000 loss 0.10172875220910646\n",
      "step 695000 loss 0.09704525017249398\n",
      "step 696000 loss 0.10009156233794056\n",
      "step 697000 loss 0.09749661900079809\n",
      "step 698000 loss 0.10031347961910068\n",
      "step 699000 loss 0.10891462279995903\n",
      "error 699834 CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 2.53 GiB already allocated; 36.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 700000 loss 0.10051843978487887\n",
      "step 701000 loss 0.10035187427024357\n",
      "step 702000 loss 0.09984269932261668\n",
      "step 703000 loss 0.10107358976639808\n",
      "step 704000 loss 0.10028015581821091\n",
      "step 705000 loss 0.09462349191587419\n",
      "step 706000 loss 0.10739323132671416\n",
      "step 707000 loss 0.10734739301877562\n",
      "step 708000 loss 0.10089190659555607\n",
      "step 709000 loss 0.11007152747502551\n",
      "step 710000 loss 0.10757698653521948\n",
      "error 710114 CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 4.00 GiB total capacity; 2.56 GiB already allocated; 30.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 711000 loss 0.11026732369652018\n",
      "step 712000 loss 0.09733951160986908\n",
      "step 713000 loss 0.10017868120456114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 714000 loss 0.09866076239780523\n",
      "step 715000 loss 0.10140288830338977\n",
      "step 716000 loss 0.1028235488627106\n",
      "error 716542 CUDA out of memory. Tried to allocate 140.00 MiB (GPU 0; 4.00 GiB total capacity; 2.50 GiB already allocated; 108.20 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 717000 loss 0.09861275741481222\n",
      "error 717311 CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 4.00 GiB total capacity; 1.92 GiB already allocated; 60.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 718000 loss 0.10642920645163395\n",
      "step 719000 loss 0.09960286799632014\n",
      "step 720000 loss 0.09978142785839736\n",
      "error 720335 CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 4.00 GiB total capacity; 2.59 GiB already allocated; 20.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 721000 loss 0.10479881280846894\n",
      "step 722000 loss 0.10009151414618828\n",
      "step 723000 loss 0.10621841505076736\n",
      "step 724000 loss 0.108048835020978\n",
      "step 725000 loss 0.10037214457103982\n",
      "step 726000 loss 0.09958955318410881\n",
      "step 727000 loss 0.09891553634172305\n",
      "step 728000 loss 0.09807082614745014\n",
      "error 728120 CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 4.00 GiB total capacity; 2.54 GiB already allocated; 50.20 MiB free; 2.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 728242 CUDA out of memory. Tried to allocate 184.00 MiB (GPU 0; 4.00 GiB total capacity; 2.29 GiB already allocated; 142.20 MiB free; 2.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 729000 loss 0.10273403442022391\n",
      "step 730000 loss 0.10437556397076696\n",
      "error 730968 CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 4.00 GiB total capacity; 2.58 GiB already allocated; 22.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 731000 loss 0.10349253083555959\n",
      "step 732000 loss 0.09911941637285054\n",
      "step 733000 loss 0.09903425958752632\n",
      "step 734000 loss 0.10170971753960475\n",
      "step 735000 loss 0.09955771067063324\n",
      "step 736000 loss 0.09736391076189466\n",
      "error 736218 CUDA out of memory. Tried to allocate 296.00 MiB (GPU 0; 4.00 GiB total capacity; 2.32 GiB already allocated; 250.20 MiB free; 2.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 737000 loss 0.1093762763699051\n",
      "step 738000 loss 0.09868705727276392\n",
      "step 739000 loss 0.10561150177801028\n",
      "step 740000 loss 0.10005342253367416\n",
      "step 741000 loss 0.09437204732745885\n",
      "error 741236 CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 4.00 GiB total capacity; 2.54 GiB already allocated; 60.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 742000 loss 0.09740889825328486\n",
      "step 743000 loss 0.09772269072965718\n",
      "step 744000 loss 0.09295790619635955\n",
      "step 745000 loss 0.10037320768868085\n",
      "step 746000 loss 0.0931424890141352\n",
      "step 747000 loss 0.10235403262451291\n",
      "step 748000 loss 0.09061522026313469\n",
      "step 749000 loss 0.10371461689099669\n",
      "error 749860 CUDA out of memory. Tried to allocate 252.00 MiB (GPU 0; 4.00 GiB total capacity; 2.39 GiB already allocated; 6.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 749921 CUDA out of memory. Tried to allocate 202.00 MiB (GPU 0; 4.00 GiB total capacity; 2.40 GiB already allocated; 186.20 MiB free; 2.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 750000 loss 0.09929592542280442\n",
      "step 751000 loss 0.09974675648496487\n",
      "step 752000 loss 0.10415618358086795\n",
      "step 753000 loss 0.09818168954807334\n",
      "step 754000 loss 0.09710040013631806\n",
      "step 755000 loss 0.09794124977826141\n",
      "step 756000 loss 0.09641602794267237\n",
      "step 757000 loss 0.09315731069794855\n",
      "step 758000 loss 0.09473808761895634\n",
      "error 758377 CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 4.00 GiB total capacity; 2.01 GiB already allocated; 24.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 759000 loss 0.09715846242802217\n",
      "error 759508 CUDA out of memory. Tried to allocate 88.00 MiB (GPU 0; 4.00 GiB total capacity; 2.59 GiB already allocated; 10.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 760000 loss 0.10360723105631768\n",
      "step 761000 loss 0.10465582302107941\n",
      "step 762000 loss 0.0988707338036038\n",
      "step 763000 loss 0.09924292037310078\n",
      "step 764000 loss 0.09615453841118142\n",
      "step 765000 loss 0.094895762210479\n",
      "step 766000 loss 0.09908511696662754\n",
      "step 767000 loss 0.09573379181465135\n",
      "step 768000 loss 0.10311925604753196\n",
      "step 769000 loss 0.09637468684418127\n",
      "step 770000 loss 0.091228292302927\n",
      "step 771000 loss 0.09352099263574928\n",
      "error 771274 CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 4.00 GiB total capacity; 2.53 GiB already allocated; 54.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 772000 loss 0.10225204186385962\n",
      "step 773000 loss 0.09986193409655243\n",
      "step 774000 loss 0.08917021653568372\n",
      "step 775000 loss 0.09822609251807443\n",
      "step 776000 loss 0.09692953793937341\n",
      "step 777000 loss 0.09689124729228206\n",
      "step 778000 loss 0.09960506393667311\n",
      "step 779000 loss 0.0924151937298011\n",
      "error 779607 CUDA out of memory. Tried to allocate 106.00 MiB (GPU 0; 4.00 GiB total capacity; 2.54 GiB already allocated; 12.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 780000 loss 0.09641371418721974\n",
      "step 781000 loss 0.09765567784942687\n",
      "step 782000 loss 0.09758728077681735\n",
      "step 783000 loss 0.09789488937449642\n",
      "step 784000 loss 0.08869566798838786\n",
      "step 785000 loss 0.09305774649349041\n",
      "step 786000 loss 0.09342567311157472\n",
      "step 787000 loss 0.09163040014612489\n",
      "step 788000 loss 0.09377272931998595\n",
      "step 789000 loss 0.09704074316238985\n",
      "step 790000 loss 0.09704037260939367\n",
      "step 791000 loss 0.08953683236474172\n",
      "step 792000 loss 0.09619912327756173\n",
      "step 793000 loss 0.09205880724126474\n",
      "error 793958 CUDA out of memory. Tried to allocate 140.00 MiB (GPU 0; 4.00 GiB total capacity; 2.48 GiB already allocated; 68.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 794000 loss 0.09624359480664134\n",
      "step 795000 loss 0.10468623705557548\n",
      "step 796000 loss 0.10029832652444019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 797000 loss 0.09978106440638658\n",
      "step 798000 loss 0.09857285513286479\n",
      "error 798454 CUDA out of memory. Tried to allocate 232.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 120.20 MiB free; 2.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 799000 loss 0.09432505177450366\n",
      "step 800000 loss 0.09481737771281042\n",
      "step 801000 loss 0.09354269078047946\n",
      "step 802000 loss 0.09058417246723548\n",
      "step 803000 loss 0.09838391212245914\n",
      "step 804000 loss 0.09397814438841305\n",
      "step 805000 loss 0.10082559155044146\n",
      "step 806000 loss 0.09214344577700831\n",
      "step 807000 loss 0.09286327820550651\n",
      "step 808000 loss 0.0978928542973008\n",
      "error 808036 CUDA out of memory. Tried to allocate 106.00 MiB (GPU 0; 4.00 GiB total capacity; 2.44 GiB already allocated; 98.20 MiB free; 2.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 809000 loss 0.0955895432382822\n",
      "step 810000 loss 0.09795831942046061\n",
      "error 810337 CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 4.00 GiB total capacity; 2.60 GiB already allocated; 18.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 811000 loss 0.09365924747916869\n",
      "step 812000 loss 0.09520596290775575\n",
      "step 813000 loss 0.09271125138807111\n",
      "step 814000 loss 0.09251489872275852\n",
      "step 815000 loss 0.09708549885754474\n",
      "step 816000 loss 0.09332440850464627\n",
      "step 817000 loss 0.09171511881542392\n",
      "step 818000 loss 0.08951864018535707\n",
      "step 819000 loss 0.09254835882596671\n",
      "error 819637 CUDA out of memory. Tried to allocate 340.00 MiB (GPU 0; 4.00 GiB total capacity; 2.36 GiB already allocated; 240.20 MiB free; 2.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 820000 loss 0.09482304008328356\n",
      "step 821000 loss 0.0922054930953309\n",
      "error 821612 CUDA out of memory. Tried to allocate 110.00 MiB (GPU 0; 4.00 GiB total capacity; 2.51 GiB already allocated; 30.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 822000 loss 0.09037322181998753\n",
      "step 823000 loss 0.09428952827001921\n",
      "step 824000 loss 0.09421112971939147\n",
      "step 825000 loss 0.09955381928547286\n",
      "step 826000 loss 0.09439834480686113\n",
      "step 827000 loss 0.09867568034119904\n",
      "step 828000 loss 0.08869503860059194\n",
      "step 829000 loss 0.09895518456073478\n",
      "step 830000 loss 0.09055204976175446\n",
      "step 831000 loss 0.09446970613673329\n",
      "step 832000 loss 0.08735359812201932\n",
      "step 833000 loss 0.09886179265216924\n",
      "step 834000 loss 0.09776131183374673\n",
      "step 835000 loss 0.08586805382499006\n",
      "step 836000 loss 0.09277665986749344\n",
      "step 837000 loss 0.09174189155129715\n",
      "error 837644 CUDA out of memory. Tried to allocate 500.00 MiB (GPU 0; 4.00 GiB total capacity; 2.03 GiB already allocated; 214.20 MiB free; 2.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 838000 loss 0.0983122154327575\n",
      "step 839000 loss 0.08868668861011975\n",
      "step 840000 loss 0.0912202822464751\n",
      "step 841000 loss 0.08800224771047942\n",
      "error 841592 CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 4.00 GiB total capacity; 2.47 GiB already allocated; 56.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 842000 loss 0.09492459728696849\n",
      "step 843000 loss 0.09147456630098168\n",
      "step 844000 loss 0.09078308018064127\n",
      "step 845000 loss 0.09457997369091027\n",
      "step 846000 loss 0.0977515850837808\n",
      "step 847000 loss 0.09201901838229969\n",
      "error 847566 CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 4.00 GiB total capacity; 2.56 GiB already allocated; 24.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 848000 loss 0.08831985761690885\n",
      "step 849000 loss 0.09513223130558618\n",
      "error 849432 CUDA out of memory. Tried to allocate 318.00 MiB (GPU 0; 4.00 GiB total capacity; 2.15 GiB already allocated; 186.20 MiB free; 2.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 850000 loss 0.08850810231664218\n",
      "step 851000 loss 0.08938181571965106\n",
      "step 852000 loss 0.08905477967462502\n",
      "error 852628 CUDA out of memory. Tried to allocate 176.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 124.20 MiB free; 2.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 853000 loss 0.0918405348036904\n",
      "step 854000 loss 0.08988080757646821\n",
      "step 855000 loss 0.09688611216819845\n",
      "step 856000 loss 0.08909047902352177\n",
      "step 857000 loss 0.103455895776744\n",
      "step 858000 loss 0.09128226668178104\n",
      "step 859000 loss 0.09355770391691476\n",
      "step 860000 loss 0.09770656955766026\n",
      "step 861000 loss 0.09795580388465897\n",
      "step 862000 loss 0.09327743831393309\n",
      "step 863000 loss 0.09599240759550594\n",
      "error 863235 CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 2.53 GiB already allocated; 28.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 864000 loss 0.09294779961579479\n",
      "step 865000 loss 0.09342637650086544\n",
      "error 865335 CUDA out of memory. Tried to allocate 612.00 MiB (GPU 0; 4.00 GiB total capacity; 2.29 GiB already allocated; 280.20 MiB free; 2.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 866000 loss 0.08848980128765106\n",
      "step 867000 loss 0.08827834722492844\n",
      "step 868000 loss 0.09413421389763243\n",
      "step 869000 loss 0.09418470234517008\n",
      "error 869748 CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 4.00 GiB total capacity; 2.51 GiB already allocated; 12.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 870000 loss 0.08753355786018073\n",
      "step 871000 loss 0.08904165663081222\n",
      "step 872000 loss 0.09361241294257343\n",
      "step 873000 loss 0.08955296665802598\n",
      "step 874000 loss 0.10167943830275908\n",
      "step 875000 loss 0.08939054862433113\n",
      "step 876000 loss 0.09297121465369128\n",
      "step 877000 loss 0.088223865382839\n",
      "step 878000 loss 0.09538861562497913\n",
      "step 879000 loss 0.08258470269024838\n",
      "step 880000 loss 0.09488963758363389\n",
      "error 880018 CUDA out of memory. Tried to allocate 194.00 MiB (GPU 0; 4.00 GiB total capacity; 2.49 GiB already allocated; 118.20 MiB free; 2.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 881000 loss 0.084024527974776\n",
      "error 881618 CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 80.20 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 882000 loss 0.08650071184360422\n",
      "error 882880 CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 2.54 GiB already allocated; 38.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 883000 loss 0.08501373364892788\n",
      "step 884000 loss 0.09419221998809371\n",
      "step 885000 loss 0.09134126584057231\n",
      "error 885679 CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 2.55 GiB already allocated; 14.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 886000 loss 0.08886762151622679\n",
      "step 887000 loss 0.0907416554717347\n",
      "error 887342 CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 4.00 GiB total capacity; 2.50 GiB already allocated; 8.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 888000 loss 0.08821680927183478\n",
      "step 889000 loss 0.09160979058896192\n",
      "step 890000 loss 0.08915924418298528\n",
      "step 891000 loss 0.09560173292434775\n",
      "error 891126 CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 4.00 GiB total capacity; 2.37 GiB already allocated; 80.20 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 891938 CUDA out of memory. Tried to allocate 6.26 GiB (GPU 0; 4.00 GiB total capacity; 271.06 MiB already allocated; 2.34 GiB free; 314.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 892000 loss 0.09524152418877929\n",
      "error 892707 CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 4.00 GiB total capacity; 2.58 GiB already allocated; 46.20 MiB free; 2.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 893000 loss 0.09442365524952766\n",
      "error 893241 CUDA out of memory. Tried to allocate 264.00 MiB (GPU 0; 4.00 GiB total capacity; 2.40 GiB already allocated; 192.20 MiB free; 2.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 893816 CUDA out of memory. Tried to allocate 382.00 MiB (GPU 0; 4.00 GiB total capacity; 2.44 GiB already allocated; 12.20 MiB free; 2.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 894000 loss 0.0969833386283135\n",
      "error 894419 CUDA out of memory. Tried to allocate 354.00 MiB (GPU 0; 4.00 GiB total capacity; 2.27 GiB already allocated; 176.20 MiB free; 2.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 895000 loss 0.08468862569215707\n",
      "step 896000 loss 0.0885121208942146\n",
      "error 896776 CUDA out of memory. Tried to allocate 4.38 GiB (GPU 0; 4.00 GiB total capacity; 244.95 MiB already allocated; 2.29 GiB free; 370.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 897000 loss 0.09017985012836288\n",
      "error 897169 CUDA out of memory. Tried to allocate 304.00 MiB (GPU 0; 4.00 GiB total capacity; 2.07 GiB already allocated; 292.20 MiB free; 2.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 897683 CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 4.00 GiB total capacity; 2.56 GiB already allocated; 22.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 898000 loss 0.09097236765990965\n",
      "step 899000 loss 0.09086283740354702\n",
      "step 900000 loss 0.0902101919001434\n",
      "step 901000 loss 0.09127979956474155\n",
      "step 902000 loss 0.08577161346632056\n",
      "step 903000 loss 0.09282427434390411\n",
      "step 904000 loss 0.09071014651260338\n",
      "error 904612 CUDA out of memory. Tried to allocate 242.00 MiB (GPU 0; 4.00 GiB total capacity; 2.31 GiB already allocated; 140.20 MiB free; 2.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 905000 loss 0.0897968310166616\n",
      "step 906000 loss 0.0838810767333489\n",
      "step 907000 loss 0.09099263524543494\n",
      "error 907020 CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 4.00 GiB total capacity; 1.78 GiB already allocated; 142.20 MiB free; 2.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 908000 loss 0.09454597471514717\n",
      "step 909000 loss 0.08924089867062866\n",
      "step 910000 loss 0.08531607468356378\n",
      "error 910372 CUDA out of memory. Tried to allocate 696.00 MiB (GPU 0; 4.00 GiB total capacity; 2.24 GiB already allocated; 400.20 MiB free; 2.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 911000 loss 0.09326067847805097\n",
      "step 912000 loss 0.0883630094495602\n",
      "error 912447 CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 64.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 912625 CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 4.00 GiB total capacity; 2.35 GiB already allocated; 86.20 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 913000 loss 0.08446066230465658\n",
      "step 914000 loss 0.09194765839888715\n",
      "step 915000 loss 0.0833156537269242\n",
      "error 915944 CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 4.00 GiB total capacity; 2.45 GiB already allocated; 70.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 916000 loss 0.09476105329697021\n",
      "step 917000 loss 0.08707005963055417\n",
      "error 917368 CUDA out of memory. Tried to allocate 320.00 MiB (GPU 0; 4.00 GiB total capacity; 2.38 GiB already allocated; 216.20 MiB free; 2.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 918000 loss 0.08905044937250205\n",
      "step 919000 loss 0.08260684793698601\n",
      "step 920000 loss 0.0859541749318596\n",
      "step 921000 loss 0.08907185459521133\n",
      "step 922000 loss 0.08701125631178729\n",
      "step 923000 loss 0.0947035586195998\n",
      "step 924000 loss 0.08296279650274664\n",
      "step 925000 loss 0.08499792508129031\n",
      "step 926000 loss 0.08508442418673076\n",
      "error 926766 CUDA out of memory. Tried to allocate 88.00 MiB (GPU 0; 4.00 GiB total capacity; 2.52 GiB already allocated; 80.20 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 927000 loss 0.084660089220386\n",
      "error 927377 CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 4.00 GiB total capacity; 2.30 GiB already allocated; 284.20 MiB free; 2.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 928000 loss 0.08278056111210026\n",
      "step 929000 loss 0.08946197853679769\n",
      "step 930000 loss 0.08540668199257925\n",
      "step 931000 loss 0.08112962641462218\n",
      "step 932000 loss 0.09018070698645897\n",
      "step 933000 loss 0.09070602148619947\n",
      "step 934000 loss 0.08938274859683588\n",
      "step 935000 loss 0.08740863130311481\n",
      "step 936000 loss 0.08085799121065065\n",
      "step 937000 loss 0.08702542889548931\n",
      "error 937567 CUDA out of memory. Tried to allocate 102.00 MiB (GPU 0; 4.00 GiB total capacity; 2.44 GiB already allocated; 62.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 938000 loss 0.08480368027789519\n",
      "step 939000 loss 0.09093534918036311\n",
      "step 940000 loss 0.09144979714672082\n",
      "step 941000 loss 0.08345345995854586\n",
      "step 942000 loss 0.08446130857337267\n",
      "error 942134 CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 4.00 GiB total capacity; 2.56 GiB already allocated; 48.20 MiB free; 2.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 943000 loss 0.08889115385059267\n",
      "step 944000 loss 0.0874285306839738\n",
      "error 944032 CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 4.00 GiB total capacity; 2.39 GiB already allocated; 104.20 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 944614 CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 4.00 GiB total capacity; 2.51 GiB already allocated; 88.20 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 945000 loss 0.08311114161880687\n",
      "error 945242 CUDA out of memory. Tried to allocate 564.00 MiB (GPU 0; 4.00 GiB total capacity; 2.13 GiB already allocated; 494.20 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 945321 CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 4.00 GiB total capacity; 2.51 GiB already allocated; 44.20 MiB free; 2.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 946000 loss 0.09017172969807871\n",
      "step 947000 loss 0.08922084815963172\n",
      "step 948000 loss 0.08700838425196707\n",
      "error 948806 CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 4.00 GiB total capacity; 2.48 GiB already allocated; 54.20 MiB free; 2.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 949000 loss 0.08943591048941016\n",
      "step 950000 loss 0.08326188087021001\n",
      "step 951000 loss 0.08669867074047215\n",
      "step 952000 loss 0.08851581722334959\n",
      "step 953000 loss 0.08362778428499587\n",
      "step 954000 loss 0.08781712173088453\n",
      "step 955000 loss 0.08804215038067195\n",
      "step 956000 loss 0.09016953129216564\n",
      "error 956154 CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 4.00 GiB total capacity; 2.56 GiB already allocated; 10.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 956849 CUDA out of memory. Tried to allocate 8.14 GiB (GPU 0; 4.00 GiB total capacity; 293.65 MiB already allocated; 2.34 GiB free; 318.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 957000 loss 0.08675746219884604\n",
      "error 957428 CUDA out of memory. Tried to allocate 106.00 MiB (GPU 0; 4.00 GiB total capacity; 2.53 GiB already allocated; 26.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 957476 CUDA out of memory. Tried to allocate 184.00 MiB (GPU 0; 4.00 GiB total capacity; 2.49 GiB already allocated; 8.20 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 958000 loss 0.08887549167615362\n",
      "step 959000 loss 0.08501457736850716\n",
      "error 959837 CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 4.00 GiB total capacity; 2.49 GiB already allocated; 26.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 960000 loss 0.0939317255828064\n",
      "error 960170 CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 4.00 GiB total capacity; 2.50 GiB already allocated; 74.20 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 961000 loss 0.08342162620043382\n",
      "step 962000 loss 0.08472747218457516\n",
      "error 962001 CUDA out of memory. Tried to allocate 178.00 MiB (GPU 0; 4.00 GiB total capacity; 2.41 GiB already allocated; 42.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "error 962271 CUDA out of memory. Tried to allocate 230.00 MiB (GPU 0; 4.00 GiB total capacity; 2.21 GiB already allocated; 196.20 MiB free; 2.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 963000 loss 0.08401228824979626\n",
      "step 964000 loss 0.08776638467120938\n",
      "step 965000 loss 0.08625713039224502\n",
      "step 966000 loss 0.07917827196256258\n",
      "step 967000 loss 0.0807944815501105\n",
      "error 967044 CUDA out of memory. Tried to allocate 132.00 MiB (GPU 0; 4.00 GiB total capacity; 1.85 GiB already allocated; 24.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 968000 loss 0.09860999933583661\n",
      "step 969000 loss 0.08496413048822432\n",
      "step 970000 loss 0.08107916441431735\n",
      "step 971000 loss 0.08665796897758264\n",
      "error 971224 CUDA out of memory. Tried to allocate 564.00 MiB (GPU 0; 4.00 GiB total capacity; 2.12 GiB already allocated; 494.20 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 972000 loss 0.08625023336266167\n",
      "step 973000 loss 0.08739047346101142\n",
      "error 973618 CUDA out of memory. Tried to allocate 234.00 MiB (GPU 0; 4.00 GiB total capacity; 2.25 GiB already allocated; 160.20 MiB free; 2.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 974000 loss 0.08481518008059356\n",
      "error 974165 CUDA out of memory. Tried to allocate 136.00 MiB (GPU 0; 4.00 GiB total capacity; 2.43 GiB already allocated; 24.20 MiB free; 2.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 975000 loss 0.08928496128052939\n",
      "step 976000 loss 0.08084241666365415\n",
      "step 977000 loss 0.09004547361284494\n",
      "step 978000 loss 0.08542263434198685\n",
      "step 979000 loss 0.09093589480081574\n",
      "step 980000 loss 0.09141773560782894\n",
      "step 981000 loss 0.07908525465824641\n",
      "step 982000 loss 0.08365372589917387\n",
      "step 983000 loss 0.08568151694175322\n",
      "step 984000 loss 0.08058231674996205\n",
      "step 985000 loss 0.08335229357751087\n",
      "error 985756 CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 2.56 GiB already allocated; 42.20 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 986000 loss 0.0830312908811029\n",
      "step 987000 loss 0.08292285599024035\n",
      "error 987847 CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 4.00 GiB total capacity; 2.54 GiB already allocated; 66.20 MiB free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 988000 loss 0.0803523791359039\n",
      "step 989000 loss 0.08337444213300478\n",
      "error 989535 CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 4.00 GiB total capacity; 2.42 GiB already allocated; 102.20 MiB free; 2.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "step 990000 loss 0.08616153171809855\n",
      "step 991000 loss 0.08324028142844327\n",
      "step 992000 loss 0.08261146263848058\n",
      "step 993000 loss 0.07597875933954493\n",
      "step 994000 loss 0.08326002162729855\n",
      "step 995000 loss 0.08337767101055943\n",
      "step 996000 loss 0.0841930261477828\n",
      "step 997000 loss 0.08529360273852944\n",
      "step 998000 loss 0.08239811799349263\n",
      "step 999000 loss 0.08979001031816006\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for i in trange(1_000_000):\n",
    "    r = random.random()\n",
    "    if r < share_real:\n",
    "        batch = df_train.sample(bs)\n",
    "        xx, yy = batch.trash2.tolist(), batch.clean2.tolist()\n",
    "    elif r < share_real + share_noiser:\n",
    "        yy = random.sample(clean_sents, bs)\n",
    "        xx = [noiser.add_noise(text, edit_rate=0.05) if random.random() > p_keep else text for text in yy]\n",
    "    else:\n",
    "        yy = random.sample(clean_sents, bs)\n",
    "        xx = [add_simple_noise(text, all_chars, edit_rate=0.05) if random.random() > p_keep else text for text in yy]\n",
    "    \n",
    "    try:\n",
    "        x = tokenizer(xx, padding=True, return_tensors='pt').to(model.device)\n",
    "        y = tokenizer(yy, padding=True, return_tensors='pt').to(model.device)\n",
    "\n",
    "        y.input_ids[y.input_ids == 0] = -100\n",
    "        loss = model(\n",
    "            input_ids=x.input_ids,\n",
    "            attention_mask=x.attention_mask,\n",
    "            labels=y.input_ids,\n",
    "            decoder_attention_mask=y.attention_mask,\n",
    "            return_dict=True\n",
    "        ).loss\n",
    "        loss.backward()\n",
    "        if i % grad_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "    except RuntimeError as e:\n",
    "        loss = None\n",
    "        optimizer.zero_grad()\n",
    "        cleanup()\n",
    "        print('error', i, e)\n",
    "        \n",
    "    if i % report_steps == 0:\n",
    "        print('step', i, 'loss', np.mean(losses[-report_steps:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27df9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff7e924e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3790ae976940758cd37e458074118b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_small['fixed2'] = [fix(text, num_beams=2) for text in tqdm(dev_small.trash2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e0a36a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distance               1.2900\n",
       "normalized_distance    0.0175\n",
       "edit_max_cldiff        0.4600\n",
       "edit_max_lendiff       0.0300\n",
       "change_amount          1.0200\n",
       "new_diff               1.6300\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_small['change_amount'] = dev_small.apply(lambda row: textdistance.levenshtein.distance(row.trash2, row.fixed2), axis=1)\n",
    "dev_small['new_diff'] = dev_small.apply(lambda row: textdistance.levenshtein.distance(row.clean2, row.fixed2), axis=1)\n",
    "\n",
    "dev_small.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af50f3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "0.21705426356589153\n"
     ]
    }
   ],
   "source": [
    "cnd = dev_small.new_diff * (dev_small.change_amount < 5) + dev_small.distance * (dev_small.change_amount >= 5)\n",
    "print(cnd.sum())\n",
    "print(1 - cnd.sum() / dev_small.distance.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ccb9f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../models/t5-tiny-denoise-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ac8fa5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/t5-tiny-denoise-v2\\\\tokenizer_config.json',\n",
       " '../models/t5-tiny-denoise-v2\\\\special_tokens_map.json',\n",
       " '../models/t5-tiny-denoise-v2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(path)\n",
    "tokenizer.save_pretrained(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
